{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4-classificadores_lineares.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/geocarvalho/uni-proj/blob/master/IF699/cleber/4-classificadores_lineares.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljw0Aw7oY7_G",
        "colab_type": "text"
      },
      "source": [
        "# Classificadores lineares\n",
        "\n",
        "* Sabe-se que se formarmos uma **combinação linear de duas variáveis**, e igualá-la a um número, então os pontos no espaço bidimensional podem ser divididos em três categorias:\n",
        "\n",
        "> 1. Pontos **pertencentes à linha** com coordenadas tais que $w_1.x_1 + w_2.x_2 = \\theta$\n",
        "> 2. Pontos **em um lado da linha** com coordenadas tais que $w_1.x_1 + w_2.x_2 < \\theta$\n",
        "> 3. Pontos **no outro lado da linha** com coordenadas tais que $w_1.x_1 + w_2.x_2 > \\theta$\n",
        "\n",
        "## Modelos lineares\n",
        "\n",
        "* Pense em **exemplos de treinamento** como pontos em um espaço n-dimensional. Cada dimensão corresponder a uma *feature*;\n",
        "\n",
        "* Um classificador binário define um plano no espaço que separa exemplos **positivos** dos **negativos**.\n",
        "\n",
        "## Fronteira de decisão linear\n",
        "\n",
        "* Um hiper-plano é a generalização de uma linha reta para mais de duas dimensões;\n",
        "\n",
        "* Um hiper-plano contém todos os pontos de um espaço n-deminesional satisfazendo a equação $w_1x_1 + w_2x_2 + , ... , w_dx_d + w_0 = 0$\n",
        "\n",
        "* Cada coeficiente w_i pode ser pensado como o peso para uma *feature* correspondente;\n",
        "\n",
        "* O vetor que contém todos os pesos $\\mathbf{w} = (w_0, ..., w_d)$ é o **vetor parâmetro** ou **vetor peso**.\n",
        "\n",
        "## Vetor normal\n",
        "\n",
        "* Geometricamente, o vetor peso $\\mathbf{w}$ é um **vetor normal** do hiper-plano separado. Um **vetor normal** de uma superfície é qualquer vetor no qual é perpendicular a ela.\n",
        "\n",
        "![vetor_normal](vetor_normal.png)\n",
        "\n",
        "## Hiper-plano como classificador\n",
        "\n",
        "$g(\\mathbf{x}) = w_1x_1 + w_2x_2, ..., w_dx_d + w_0$\n",
        "\n",
        "$y = sign(g(\\mathbf{x})) = \n",
        "  \\begin{cases}\n",
        "  +1 & \\quad se\\ g(x) \\geq 0 \\\\\n",
        "  -1 & \\quad senão\n",
        "  \\end{cases}$\n",
        "\n",
        "## Bias\n",
        "\n",
        "* A inclinação do hiper-plano é determinada por $w_1 ... w_d$ e a localização (interceptação) é determinada pelo bias (tendência) $w_0$;\n",
        "\n",
        "* Incluimos o bias no vetor peso e adicionamos um componente modelo para o vetor *feture* ($x_0=1$).\n",
        "\n",
        "$g(x) = \\sum^d_{i=0}w_i . x_i$\n",
        "\n",
        "$g(x) = \\mathbf{w} . \\mathbf{x}$\n",
        "\n",
        "## Separando hiper-planos em duas dimensões\n",
        "\n",
        "![3_lines](3_lines.png)\n",
        "\n",
        "## Exemplo\n",
        "\n",
        "* Vantagens de separabilidade linear\n",
        "\n",
        "![easy](easy_linear.png)\n",
        "\n",
        "* Como separar as duas classes com apenas um ponto?\n",
        "\n",
        "![not_easy](not_easy_linear.png)\n",
        "\n",
        "* Possível solução é aumentar a quantidade de dimensões (**Teorema de Cover**, 1965)\n",
        "\n",
        "$\\Phi(X_1) = (X_1, X^2_1)$\n",
        "\n",
        "![table_graph](table_graph.png)\n",
        "\n",
        "## Método paramétrico\n",
        "\n",
        "* Função recebe imagem e parâmetros resultando em 10 números indicando as pontuações das classes;\n",
        "\n",
        "$f(x_i, W) = Wx_i + b$\n",
        "\n",
        "> Onde os pesos $W$ alteram o ângulo por exemplo, enquanto o bias $b$ altera somente a posição. \n",
        "\n",
        "![slide_17](slide_17)\n",
        "\n",
        "* Quais classes seriam difíceis para um classificador linear distinguir?\n",
        "\n",
        "> Exemplo s com 3 imagens (gato, carro, rã) com $W$ aleatório;\n",
        "\n",
        "> Primeiro passo é definir uma **função de perda** que quantifica nossa infelicidade com as pontuações dos dados de treino;\n",
        "\n",
        "> Depois encontrar uma forma eficiente de encontrar parâmetros que minimizem a função de perda (**otimização**).\n",
        "\n",
        "* Dado um exemplo $(x_i, y_i)$ onde $x_i$ é a imagem e o $y_i$ é o rótulo, usando a função para o vetor de pontuações $s = f(x_i, W)$ a função de perda para um SVM é\n",
        "\n",
        "$L_i = \\sum_{j \\neq y_i}max(0, s_j - s_{y_i} + 1)$\n",
        "\n",
        "* E a perda total no treinamento é a média entre todos os exemplos nos dados de treinamento.\n",
        "\n",
        "$L = \\frac{1}{N}\\sum^N_{i=1}L_i$\n",
        "\n",
        "* Ficando:\n",
        "\n",
        "$L = \\frac{1}{N}\\sum^N_{i=1}\\sum_{j \\neq y_i}max(0,f(x_i;W)_j - f(x_i;W)_{y_i} + 1)$\n",
        "\n",
        "> **Temos um bug para W dado L=0, temos mais de um W ótimo e isso não é uma propriedade boa. Para preferir um W invés de outro, usamos regularização.**\n",
        "\n",
        "* Existem infinitos hiperplanos que separam dois conjuntos de pontos linearmente separáveis. Qual o melhor?\n",
        "\n",
        "> Menor erro de classificação e maior margem.\n",
        "\n",
        "## Peso por regularização\n",
        "\n",
        "$L = \\frac{1}{N}\\sum^N_{i=1}\\sum_{j \\neq y_i}max(0,f(x_i;W)_j - f(x_i;W)_{y_i} + 1) + \\lambda R(W)$\n",
        "\n",
        "* É normal usar:\n",
        "\n",
        "> **Regularização L2** (Euclidean?)  $R(W) = \\sum_k\\sum_lW^2_{k,l}$\n",
        "\n",
        "> Regularização L1 (Manhattan) $R(W) = \\sum_k\\sum_l|W^2_{k,l}|$\n",
        "\n",
        "> Elastic net (L1 + L2) $R(W) = \\sum_k\\sum_l \\beta W^2_{k,l} + |W_{k,l}|$\n",
        "\n",
        "> Outras alternativas são: regularização Max Norm e Dropout\n",
        "\n",
        "## Classificador softmax: Regressão logística multinomial\n",
        "\n",
        "$P(Y=k | X=x_i) = \\frac{e^{s_k}}{\\sum_j e^{s_j}}$\n",
        "\n",
        "> Onde $s = f(x_i; W)$\n",
        "\n",
        "* Queremos maximizar o log da probabilidade ou (para a função de perda) minimizar o log negativo da probabilidade da classe correta;\n",
        "\n",
        "$L_i = -log(\\frac{e^s}{\\sum_j e^{s_j}})$\n",
        "\n",
        "![slide_47](slid_47.png)\n",
        "\n",
        "\n",
        "\n",
        "## Otimização\n",
        "\n",
        "* Temos um dataset, uma **função de pontuação** e a **função de perda**;\n",
        "\n",
        "> Softmax $L_i = -log(\\frac{e^s}{\\sum_j e^{s_j}})$\n",
        "\n",
        "> SVM $L_i = \\sum_{j \\neq y_i}max(0, s_j - s_{y_i} + 1)$\n",
        "\n",
        "> Perda total $L = \\frac{1}{N}\\sum^N_{i=1}L_i + R(W)$\n",
        "\n",
        "* A primeira alternativa é a **pesquisa aleatória** que não é viável;\n",
        "\n",
        "* A segunda é **seguir a inclinação**:\n",
        "\n",
        "> Para uma dimensão a derivada da função $\\frac{df(x)}{dx} = lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h}$\n",
        "\n",
        "> Para múltiplas dimensões, o **gradiente** é o vetor das derivativas parciais;\n",
        "\n",
        "> É aproximado e é muito lento para avaliar;\n",
        "\n",
        "\n",
        "## Em resumo:\n",
        "\n",
        "* Gradiente numérico é aproximado, lento e fácil de escrever;\n",
        "\n",
        "* Gradiente analítico é exato, rápido e propenso a erro;\n",
        "\n",
        "* Na prática sempre usar o gradiente analíticos, mas checar a implementação com o gradiente numérico (essa revisão é chamada **gradient check**).\n",
        "\n",
        "## Mini-batch gradient descent\n",
        "\n",
        "* Na prática é o que se usa, é mais eficiente. Usa GPU.\n",
        "\n",
        "## Referências\n",
        "\n",
        "* [CS231n Winter 2016: Lecture 2: Data-driven approach, kNN, Linear Classification 1](https://www.youtube.com/watch?v=8inugqHkfvE)\n",
        "> Linear classification (26:00)\n",
        "\n",
        "* [CS231n Winter 2016: Lecture 3: Linear Classification 2, Optimization](https://www.youtube.com/watch?v=qlLChbHhbg4)\n",
        "* [CS231n: Convolutional Neural Networks for Visual Recognition](http://vision.stanford.edu/teaching/cs231n/)\n",
        "* [Python Numpy Tutorial, This tutorial was contributed by Justin Johnson.](http://cs231n.github.io/python-numpy-tutorial/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHR4LF2JRy1U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0ZrEvw9Y7Dk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "https://classroom.google.com/u/0/c/NDE5NDQxOTU1MTJa"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}