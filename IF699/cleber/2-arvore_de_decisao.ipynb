{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Árvores de decisão\n",
    "\n",
    "* Método para inferência indutiva;\n",
    "* Auxilia a predizer a classe de um objeto em estudo com base em treinamento prévio;\n",
    "* Uma árvore representa uma função discreta para aproximar/representar os dados de treinamento;\n",
    "* Árvores de decisão classificam instâncias ordenando-as da raíz para algum nó folha;\n",
    "> Cada nó da árvore representa um atributo.\n",
    "* [Uma Introducao Visual ao Aprendizado de Maquina - usar exemplo com árvode de decisão](http://www.r2d3.us/uma-introducao-visual-ao-aprendizado-de-maquina-1/)\n",
    "\n",
    "### Exemplo: Jogar tênis\n",
    "* Classifica se um determinado dia é adrequado ou não para jogar tênis;\n",
    "![jogar tenis](jogar_tenis.png)\n",
    "\n",
    "* Por exemplo:\n",
    "> 1. Tendo a instância: (Panorama=Ensolarado) e (Temperatura=Quente) e (Umidade=Alta)\n",
    "> 2. Saída: **Não**\n",
    "\n",
    "* Pode se monta uma expressão para verificar quando é possível jogar tênis:\n",
    "> (Panorama=Ensolarado) e (Umidade=Normal) ou (Panorama=Nublado) ou (Panorama=Chuvoso) e (Vento=Fraco)\n",
    "\n",
    "* Portanto podemos gerar uma árvore de decisão e depois **obter regras** que nos auxiliam a classificar instâncias nunca vistas.\n",
    "\n",
    "## Regiões de decisão\n",
    "\n",
    "![regioes de decisao](regioes_de_decisao.png)\n",
    "\n",
    "### Tipos de problemas para aplicação\n",
    "\n",
    "* Instâncias são representadas por **pares atributo-valor**: Há um conjunto fixo de atributos (ex.: Umidade) e seus valores (ex.: Alta, Nomal). Situação ideal é quando cada atributo pode assumir **poucos valores**, no entanto, as árvores de decisão também podem trabalhar com valores reais.\n",
    "\n",
    "* A função a ser aproximada tem **valores discretos**: No exemplo a função deve produzir **sim** ou **não**. Pode-se facilmente estendê-las para produzir mais de dois valores de saída. Tornam-se mais complexas e menos utilizadas abordagens que buscam produzir valores reais como saída.\n",
    "\n",
    "* Aplicações comuns: Diagnóstico de pacientes, problemas em equipamentos mecânicos e elétricos, análise de crédito.\n",
    "\n",
    "### Tipos de algoritmo\n",
    "\n",
    "* Mais conhecidos: ID3 (Quinlan, 1986) e C4.5 (Quinlan, 1993)\n",
    "\n",
    "#### Algoritmo ID3\n",
    "\n",
    "* Considere um conjunto de dados para treinamento;\n",
    "* Ele constrói a árvore em uma abordagem ***top-down*** considerando a questão: **\"Qual atributo é o mais importante e, portanto, deve ser colocado na raíz da árvore?\"**;\n",
    "* Para isso **cada atributo é testado** e sua capacidade para se tornar nó raíz avaliada;\n",
    "* Cria-se tantos **nós filhos da raíz** quantos valores possíveis esse atributo puder assumir (caso discreto);\n",
    "* **Repete-se o processo** para cada nó filho da raíz e assim sucessivamente.\n",
    "* Como avaliar qual o atributo é mais adequado? Por **entropia**\n",
    "\n",
    "#### Entropia\n",
    "\n",
    "* Propriedade da termodinâmica usada para determinar a quantidade de energia útil de um sistema qualquer;\n",
    "* Gibbs afirmou que a melhor interpretação para entropia na mecânica estatística é como uma **medida de incerteza**;\n",
    "* Claude Shannon (1948) desenvolveu o conceito de Entropia em teoria da informação;\n",
    "* Para entender considere o sistema:\n",
    "\n",
    "![entropia 1](entropia1.png \"entropia 1\")\n",
    "\n",
    "> Considere que o sistema alterou seu comportamento\n",
    "\n",
    "![entropia 2](entropia2.png \"entropia 2\")\n",
    "\n",
    "* A equação para o seu cálculo: $E = -\\sum_{i} \\sum_{j}p_{ij}log_{2}p_{ij}$\n",
    "\n",
    "> Ela mede a energia total de um sistema considerando que o sistema está no estado $i$ e ocorre uma transição para o estado $j$. O $log_2$ é usado para quantificar a Entropia em termos de **bits**.\n",
    "\n",
    "* Assim, para imagem **entropia 1**:  $E = -(1log_2(1) + 1log_2(1)) = 0$\n",
    "* E para a imagem **entropia 2**: $E = -(1log_2(1) + 0.5log_2(0.5) + 0.5log_2(0.5)) = 0.693$\n",
    "> Após modificar seu comportamento, o sistema agregou maior nível de incerteza ou energia.\n",
    "\n",
    "#### Uso de Entropia no ID3\n",
    "\n",
    "* Considere uma coleção **S** de instâncias com exemplos positivos e negativos (duas classes distintas);\n",
    "* Nesse caso, assume-se a probabilidade de se pertencer a uma das duas classes (positiva ou negativa) de **S**;\n",
    "* Entropia nesse contexto é dada por:\n",
    "\n",
    "$E(S) = -p_\\oplus log_2 p_\\oplus - p_\\ominus log_2 p_\\ominus$\n",
    "* Para ilustrar, considere o conjunto **S** com 14 exemplos de algum conceito Booleano: 9 positivos e 5 negativos;\n",
    "* A entropia desse conjunto é dada por:\n",
    "\n",
    "$E(S) = -\\frac{9}{14}log_2\\frac{9}{14}-\\frac{5}{14}log_2\\frac{5}{14} = 0.94$\n",
    "\n",
    "* Em outros casos note:\n",
    "> para [7+, 7-]\n",
    "\n",
    "$E(S) = - \\frac{7}{14}log_2\\frac{7}{14}-\\frac{7}{14}log_2\\frac{7}{14} = 0.99 ... \\approx 1$\n",
    "\n",
    "> para [0+, 14-] ou [14+, 0-]\n",
    "\n",
    "$E(S) = - \\frac{14}{14}log_2\\frac{14}{14} = 0$\n",
    "\n",
    "* **Entropia mede o nível de certeza que temos sobre um evento**\n",
    "* Podemos generalizar para mais de dois possíveis valores ou classe:\n",
    "\n",
    "$E(S) = \\sum^{c}_{i=1}-p_ilog_2p_i$\n",
    "\n",
    "* Podemos estudar diferentes sistemas com Entropia, por exemplo séries temporais.\n",
    "* Por que o uso da função **log**?\n",
    "> Pois em teoria da informação mede-se a informação proveniente de uma fonte em bits. Esse conceito também permite pedir quantos bits são necessários para codificar uma mensagem (uma palavra por exemplo).\n",
    "\n",
    "#### Ganho de informação no ID3\n",
    "\n",
    "* Após definir **entropia**, podemos definir **ganho de informação**;\n",
    "* **Ganho de informação** mede a **efetividade** de um atributo em classificar um conjunto de treinamento. **Quão bom um atributo é** para classificar um conjunto de treinamento.\n",
    "* Ganho de informação de um atributo A: É a redução na Entropia, causada pelo particionamento de exemplos de acordo com este atributo.\n",
    "\n",
    "$\\mathbf{GI}(S, A) = E(S) - \\sum_{\\upsilon\\ \\in\\ \\mathbf{Valores}(A)} \\frac{S_\\upsilon}{S}E(S_\\upsilon)$\n",
    "\n",
    "> Em que o segundo termo mede a Entropia particionando o conjunto de treinamento de acordo com o atributo A.\n",
    "\n",
    "* Logo, **GI** mede a redução na Entropia ou na incerteza ao selecionar o atributo A\n",
    "\n",
    "* Por exemplo, considere **S** um conjunto de treinamento contendo o atributo Vento (Fraco ou Forte);\n",
    "> **S** contém 14 exemplos [9+, 5-]: 6 dos exemplos positivos e 2 exemplos dos negativos são definidos por Vento=Fraco (8 no total); 3 exemplos definidos por Vento=Forte tanto na classe positiva quanto negativa (6 no total).\n",
    "\n",
    ">  O ganho de informação ao selecionar o atributo Vento para a raíz de uma árvore de decisão é dado por:\n",
    "\n",
    "$S = [9+, 5-]$\n",
    "\n",
    "$S_{fraco} \\gets [6+, 2-]$\n",
    "\n",
    "$S_{forte} \\gets [3+, 3-]$\n",
    "\n",
    "$\\mathbf{GI}(S,A) = E(S) - \\sum_{\\upsilon\\ \\in\\ \\mathbf{Valores}(A)} \\frac{S_\\upsilon}{S}E(S_\\upsilon)$\n",
    "\n",
    "$\\mathbf{GI}(S,A) = 0.94 - \\frac{8}{14}E(S_{fraco}) - \\frac{6}{14}E(S_{forte})$\n",
    "\n",
    "$E(S_{fraco}) = -\\frac{6}{8}log_2\\frac{6}{8} - \\frac{2}{8}log_2\\frac{2}{8} = 0.811$\n",
    "\n",
    "$E(S_{fraco}) = -\\frac{3}{6}log_2\\frac{3}{6} - \\frac{3}{6}log_2\\frac{3}{6} = 1.0$\n",
    "\n",
    "$\\mathbf{GI}(S,A) = 0.94 - \\frac{8}{14}0.811 - \\frac{6}{14}1.00 = 0.048$\n",
    "\n",
    "* Essa medida de ganho de informação é utilizada pelo ID3 em cada passo da geração da árvore de decisão;\n",
    "> Nesse caso reduzimos muito pouco o nível de incerteza. Logo, esse atributo é bom para a raíz da árvore? **Não**.\n",
    "\n",
    "#### Exemplo - aprender a jogar tênis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dia</th>\n",
       "      <th>panorama</th>\n",
       "      <th>temperatura</th>\n",
       "      <th>umidade</th>\n",
       "      <th>vento</th>\n",
       "      <th>jogar_tenis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Ensolarado</td>\n",
       "      <td>Quente</td>\n",
       "      <td>Alta</td>\n",
       "      <td>Fraco</td>\n",
       "      <td>Não</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Ensolarado</td>\n",
       "      <td>Quente</td>\n",
       "      <td>Alta</td>\n",
       "      <td>Forte</td>\n",
       "      <td>Não</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Nublado</td>\n",
       "      <td>Quente</td>\n",
       "      <td>Alta</td>\n",
       "      <td>Fraco</td>\n",
       "      <td>Sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>Chuvoso</td>\n",
       "      <td>Intermediária</td>\n",
       "      <td>Alta</td>\n",
       "      <td>Fraco</td>\n",
       "      <td>Sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>Chuvoso</td>\n",
       "      <td>Fria</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Fraco</td>\n",
       "      <td>Sim</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dia    panorama    temperatura umidade  vento jogar_tenis\n",
       "0   1  Ensolarado         Quente    Alta  Fraco         Não\n",
       "1   2  Ensolarado         Quente    Alta  Forte         Não\n",
       "2   3     Nublado         Quente    Alta  Fraco         Sim\n",
       "3   4     Chuvoso  Intermediária    Alta  Fraco         Sim\n",
       "4   5     Chuvoso           Fria  Normal  Fraco         Sim"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "exe = {\n",
    "    \"dia\": [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\", \"13\", \"14\"],\n",
    "    \"panorama\": [\"Ensolarado\", \"Ensolarado\", \"Nublado\", \"Chuvoso\", \"Chuvoso\", \"Chuvoso\", \"Nublado\", \"Ensolarado\", \"Ensolarado\", \"Chuvoso\", \"Ensolarado\", \"Nublado\", \"Nublado\", \"Chuvoso\"],\n",
    "    \"temperatura\": [\"Quente\", \"Quente\", \"Quente\", \"Intermediária\", \"Fria\", \"Fria\", \"Fria\", \"Intermediária\", \"Fria\", \"Intermediária\", \"Intermediária\", \"Intermediária\", \"Quente\", \"Intermediária\"],\n",
    "    \"umidade\": [\"Alta\", \"Alta\", \"Alta\", \"Alta\", \"Normal\", \"Normal\", \"Normal\", \"Alta\", \"Normal\", \"Normal\", \"Normal\", \"Alta\", \"Normal\", \"Alta\"],\n",
    "    \"vento\": [\"Fraco\", \"Forte\", \"Fraco\", \"Fraco\", \"Fraco\", \"Forte\", \"Forte\", \"Fraco\", \"Fraco\", \"Fraco\", \"Forte\", \"Forte\", \"Fraco\", \"Forte\"],\n",
    "    \"jogar_tenis\": [\"Não\", \"Não\", \"Sim\", \"Sim\", \"Sim\", \"Não\", \"Sim\", \"Não\", \"Sim\", \"Sim\", \"Sim\", \"Sim\", \"Sim\", \"Não\"]\n",
    "}\n",
    "exe_df = pd.DataFrame.from_dict(exe)\n",
    "exe_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Primeiro passo é calcular o ganho de informação para cada atributo:\n",
    "\n",
    "$\\mathbf{GI}(S, panorama) = 0.246$\n",
    "\n",
    "$\\mathbf{GI}(S, umidade) = 0.151$\n",
    "\n",
    "$\\mathbf{GI}(S, vento) = 0.048$\n",
    "\n",
    "$\\mathbf{GI}(S, temperatura) = 0.029$\n",
    "\n",
    "* O Atributo com maior ganho é selecionado para ser a raíz da árvore de decisão (**panorama = 0.246**)\n",
    "> É o que mais reduz o nível de incerteza. Criamos nós filhos a partir da raíz de acordo com os possíveis valores assumidos pelo atributo **panorama**\n",
    "\n",
    "* Agora que temos a raíz devemos proceder da mesma maneira para os demais ramos que surgem a partir da raíz. Em cada ramo consideramos somente os exemplos nele contigos, desde que haja divergência entre as classes de saída.\n",
    "\n",
    "![panorama](panorama.png \"Panorama_slide31\")\n",
    "\n",
    "* Um dos ramos não tem divergência entre as classes de saída, ou seja, entropia é igual a zero:\n",
    "\n",
    "![panorama nublado](panorama_nublado.png \"panorama_nublado\")\n",
    "\n",
    "* Atributos existentes incorporados acima de determinado nó não entram na avaliação de **gano de informação** desese nó. Neste caso, dois novos nós serão criados, mas o atributo panorama não será mais avaliado.\n",
    "\n",
    "![panorama nublado valores](panorama_nublado_valores.png \"panorama valores\")\n",
    "\n",
    "* Computando o ganho de informação para o ramo ensolarado primeiro pelo calculo da entropia, dado $E(S=Ensolarado)$, assim temos o nível de incerteza para o ramo **panorama = ensolarado**\n",
    "\n",
    "$E(S=Ensolarado) = -\\frac{2}{5}log\\frac{2}{5} - \\frac{3}{5}log\\frac{3}{5} = 0.97$\n",
    "\n",
    "> Ganho de informação:\n",
    "\n",
    "$\\mathbf{GI}(S, umidade) = 0.97 - \\frac{3}{5}0.0 - \\frac{2}{5}0.0 = 0.97$\n",
    "\n",
    "$\\mathbf{GI}(S, temperatura) = 0.97 - \\frac{2}{5}0.0 - \\frac{2}{5}1.0 = 0.57$\n",
    "\n",
    "$\\mathbf{GI}(S, vento) = 0.97 - \\frac{2}{5}1.0 - \\frac{3}{5}0.918 = 0.019$\n",
    "\n",
    "> Escolhemos então o atributo **umidade**\n",
    "\n",
    "![panorama umidade](panorama_umidade.png)\n",
    "\n",
    "continuando\n",
    "\n",
    "![panorama umidade vento](panorama_umidade_vento.png)\n",
    "\n",
    "* O algoritmo continua até que uma das seguintes condições seja satistfeita:\n",
    "> 1. **Todos os atributos foram incluídos** no caminho da raíz até as folhas;\n",
    "> 2. Exemplos de treinamento associados com dado ramo apresentam o **mesmo valor de saída** (+ ou -)\n",
    "\n",
    "#### Sobre a busca por hipóteses do ID3\n",
    "\n",
    "* ID3 busca no espaço de hipóteses alguma adequada para representar o conjunto de treinamento. Esse espaço de hipósteses é formado por um conjunto de todas possíveis árvores de decisão.\n",
    "\n",
    "* ID3 começa com árvore vazia e progressivamente elabora hipóteses até chegar em uma árvore de decisão. A busca por hipóteses é guiada pelo **ganho de informação** dos atributos.\n",
    "\n",
    "* ID3 mantém somente uma hipótese (árvore até dado momento) durante as iterações do algoritmo.\n",
    "> Diferente do *Candidate-Elimination* que mantém o conjunto de todas hipóteses consistentes com o conjunto de treinamento.\n",
    "\n",
    "* Como ID3 não mantém todas as hipóteses consistentes com o conjunto de treinamento, não possui a habilidade de determinar quantas árvores de decisão alternativa são consistentes com os dados de treinamento.\n",
    "\n",
    "* ID3 não realiza **backtracking** na busca, ou seja, uma vez que tenha selecionado um atributo, não reavalia a árvore de decisão formada. Isso pode fazer com que convirja para um ótimo local.\n",
    "\n",
    "* ID3 emprega todos os dados de treinamento em cada passo da busca por hipóteses, toma decisões estatíscas em cada passo (vantagem: menos sensível a erros em exemplos individuais).\n",
    "> Isso contrasta com técnincas como o *Find-S* e o *Candidate-Elimination* que avaliam somente um exemplo na formulação de hipóteses válidas.\n",
    "\n",
    "#### Viés indutivo do ID3\n",
    "\n",
    "* Viés indutivo ou **bias indutivo** é o quanto uma técnica generaliza o conhecimento aprendido a partir dos exemplos de treinamento para exemplos futuros nunca vistos.\n",
    "> Por exemplo, o quanto um aluno generaliza o conhecimento estudando na véspera? Há alunos que esperam que a pergunta seja exatamente idêntica à levantada em aula, alunos com alto grau de especialização não generalizam o conhecimento.\n",
    "\n",
    "* A abordagem do ID3 privilegia \n",
    "> 1. ávores mais curtas em relação as mais longas observaras (**navalha de Occam**, preferir hipóteses mais simples que representem os dados). Acredita-se que hipóteses complexas geradas para conjuntos de treinamento podem falhar para generalizar os dados nunca vistos. Em teoria da informaçao, mensagens mais curtas consomem menos recursos para serem transmitidas;\n",
    "> 2. atributos de maior ganho de informação mais próximos do topo ou raíz da árvore.\n",
    "\n",
    "#### Questões envolvidas no aprendizado usando AD\n",
    "\n",
    "* O ID3 cresce a árvore o suficiente para classificar os exemplos de treinamento. Essa parece uma estratégiz razoável, mas pode levar a problemas quando os dados apresentam ruídos ou quando o número de exemplos de treinamento é pequeno demaias para produzir uma árvore representativa.\n",
    "\n",
    "* Nessas situações o ID3 apresenta **overfitting** nos dados de treinamento;\n",
    "> Dizemos que uma hipótese apresenta overfitting aos exemplos de treinamento se **há alguma hipótese que reprensenta com menor qualidade (maior erro) os dados de treinamento, mas que apresenta melhor desempenho (menor erro) sobre instâncias nunca vistas**.\n",
    "\n",
    "* Exemplo, o algoritmo ID3 gerando árvores com maior número de nós para um conjunto de treinamento e testee de pacientes com certa doença.\n",
    "\n",
    "![high_node_trees](high_node_trees.png)\n",
    "\n",
    "> Isso geralmente ocorre devido a erros aleatórios nos exemplos de treinamento e a ruídos nos dados. Por exemplo, se adicionarmaos um exemplo de treinamento errado ao conjunto de treinamento para o problema \"Jogar Tênis\", ID3 gerará uma árvore distinta da vista anteriormente.\n",
    "\n",
    "<panorama=ensolarado, temperatura=quente, umidade=normal, vento=forte, jogar_tenis=nao>\n",
    "\n",
    "> Esse exemplo fará com que ID3 construa uma árvore mais complexa, ou seja, com mais nós. Essa nova árvore representará perfeitamente os dados de treinamento, mas falhará para dados nunca vistos e que tendem a não apresentar esse erro.\n",
    "\n",
    "* ***Overfitting*** também ocorre quando o conjunto de treinamento é pequeno e não é significativo, ou seja, não apresenta casos que futuramente serão exercitados. Como evitar?\n",
    "> Parar crescer a árvore de decisão em dado momento, antes que ela classifique perfeitamente o conjunto de treinamento; Permitir que a árvore gerada classifique perfeitamente o conjunto de treinamento, no entanto, posteriormente, essa árvore final passará por uma etapa de ***post-pruning*** (podar).\n",
    "\n",
    "* Para se definir um bom tamanho para a árvore gerada o mais comum é utilizar um conjunto de **exemplos para validação**, que seja distinto do **conjunto de treinamento**. Assim, mede-se o desempenho das árvores geradas e escolhe-se uma com tamanho adequado.\n",
    "\n",
    "* Divide-se o conjunto de dados em **treinamento** (utilizado na etapa de indução da hipótese) e **validação** (utilizado para verificar se a hipótese gerada é adequada.\n",
    "\n",
    "* Assim, caso ocorra ***overfitting***, erros mais expressivos ocorrerão no conjunto de validação (deve ser grande o suficiente para dar validade estatística a tal avaliação).\n",
    "> É comum utilizar 2/3 dos dados para treinamento e 1/3 para validação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
