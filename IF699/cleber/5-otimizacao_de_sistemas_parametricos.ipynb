{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5-otimizacao_de_sistemas_parametricos.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/geocarvalho/uni-proj/blob/master/IF699/cleber/5-otimizacao_de_sistemas_parametricos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvQv-iXMVU3g",
        "colab_type": "text"
      },
      "source": [
        "# Otimização de sistemas paramétricos\n",
        "\n",
        "## Data preprocessing\n",
        "\n",
        "`original data > zero-centered data > normalized data`\n",
        "\n",
        "* **Antes de normalizar**: Perda de classificação muito sensível a mundaças na matrix de peso, difícil de otimizar;\n",
        "  \n",
        "* **Depois da normalização**: Menos sensível a pequenas mudanças nos pesos, fácil de otimizar.\n",
        "  \n",
        "## Onde paramos anteriormente:\n",
        "\n",
        "* função de pontuação: $s = f(x; W) = W_x$\n",
        "\n",
        "* função de perda (SVM): $L_i = \\sum_{j \\neq y_i} max(0, s_j - s_{y_i} + 1)$\n",
        "\n",
        "* função de perda + regularização: $L = \\frac{1}{N} \\sum^{N}_{i=1} L_i + \\sum_k W^2_k$\n",
        "\n",
        "* queremos $\\nabla_w L$\n",
        "\n",
        "## *Batch gradient descent* \n",
        "\n",
        "* *Vanilla gradient descent* computa o gradiente da função de custo para os parâmetros para todo o dado de treinamento\n",
        "\n",
        "$\\theta = \\theta - \\eta \\times \\nabla_{\\theta}J(\\theta)$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCZ-u97oTojC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def exemplo(args):\n",
        "  for i in range(nb_epochs):\n",
        "    params_grad = evaluate_gradient(loss_function, data, params)\n",
        "    params = params - learning_rate * params_grad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AruVTM6Xo3H",
        "colab_type": "text"
      },
      "source": [
        "## *Stochastic gradient descent* (SGD)\n",
        "\n",
        "* Gradiente descedente estocástico atualiza os parametros para cada exemplo de treinamento $x(i)$ e rótulo $y(i)$:\n",
        "\n",
        "$\\theta = \\theta - \\eta \\times \\nabla_{\\theta}J(\\theta, x^{(i)}; y^{(i)})$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bd-ynCZIXW1I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def exemplo(args):\n",
        "  for i in range(nb_epochs):\n",
        "    np.random.shuffle(data)\n",
        "    for example in data:\n",
        "      params_grad = evaluate_gradient(loss_function, example, params)\n",
        "      params = params - learning_rate * params_grad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4p2jCr_tY609",
        "colab_type": "text"
      },
      "source": [
        "## *Mini-batch gradient descent*\n",
        "\n",
        "* Gradiente descendente por mini-lote escolhe o melhor dos dois mundos e atualiza para mini-lote de $n$ exemplos de treino:\n",
        "\n",
        "$\\theta = \\theta - \\eta \\times \\nabla_{\\theta}J(\\theta, x^{(i:i+n)}; y^{(i:i+n)})$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0MQf90BY6aa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def exemplo(args):\n",
        "  for i in range(nb_epochs):\n",
        "    np.random.shuffle(data)\n",
        "    for batch in get_batches(data, batch_size=50):\n",
        "      params_grad = evaluate_gradient(loss_function, batch, params)\n",
        "      params = params - learning_rate * params_grad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CrlbUuTctUM",
        "colab_type": "text"
      },
      "source": [
        "## Problemas com SGD\n",
        "\n",
        "* O que ocorre com mudanças de perda rápidas numa direção e lentas em outra? O que o gradiente descendente faz?\n",
        "\n",
        "> A função de perda tem alta **condição de número**: proporção de valores singulares grandes para pequenos da matrix Hession é largo.\n",
        "\n",
        "* O que ocorre se a função de perda tem um mínimo local ou um ponto de sela?\n",
        "\n",
        "> Gradiente zero, gradiente descendente fica travado. Pontos de sela são comuns em altas dimensões.\n",
        "\n",
        "* O gradiente vem do mini-batch por isso é pode ter alta variação (*noisy*)\n",
        "\n",
        "![noisy](noisy.png)\n",
        "\n",
        "$L(W) = \\frac{1}{N} \\sum^N_{i=1} L_i(x_i, y_i, W)$\n",
        "\n",
        "\n",
        "$\\nabla_W L(W) = \\frac{1}{N} \\sum^N_{i=1} \\nabla_WL_i(x_i, y_i, W)$\n",
        "\n",
        "## SGD + momentum\n",
        "\n",
        "* SGD $x_{t+1} = x_t - \\alpha\\nabla f(x_t)$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULZ1qW8rY0ys",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sgd(args):\n",
        "  while True:\n",
        "    dx = compute_gradient(x)\n",
        "    x += learning_rate * dx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nqmifoe1vjrj",
        "colab_type": "text"
      },
      "source": [
        "* SGD + Momentum\n",
        "\n",
        "$v_{t+1} = pv_t + \\nabla f(x_t)$\n",
        "\n",
        "$x_{t+1} = x_t - \\alpha v_{t+1}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XX7gj5gvTrQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sgd_momentum(args):\n",
        "  vx = 0\n",
        "  while True:\n",
        "    dx = compute_gradient(x)\n",
        "    vx = rho * vx + dx\n",
        "    x += learning_rate * vx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILCQDukjwQW2",
        "colab_type": "text"
      },
      "source": [
        "* Construa \"velocidade\" rodando a média dos gradientes;\n",
        "* `Rho` dá \"fricção\", tipicamente rho = .9 ou .99\n",
        "* Se distancia do mínimo local ou do ponto de sela\n",
        "\n",
        "![gradient_noise](gradient_noise.png)\n",
        "\n",
        "![momentum_update](momentum_update.png)\n",
        "\n",
        "## *Nesterov momentum*\n",
        "\n",
        "![nesterov](nesterov.png)\n",
        "\n",
        "## NAG vs. *Standard momentum*\n",
        "\n",
        "* Primeiro faça um grande puki na direção do grandiente acumulado anteriormente. Então calcule o gradiente onde você termina e faça a correção.\n",
        "\n",
        "![jump](jump.png)\n",
        "\n",
        "## De volta ao *Nesterov momentum*\n",
        "\n",
        "$v_{t+1} = \\rho v_t - \\alpha \\nabla f(x_t + \\rho v_t)$\n",
        "\n",
        "$x_{t+1} = x_t + v_{t+1}$\n",
        "\n",
        "> onde $x_t + pv_t$ normalmente queremos atualizar em termos de $x_t, \\nabla f(x_t)$.\n",
        "\n",
        "* Mudamos as variáveis $\\tilde{x}_t + x-t + pv_t$ e rearranjamos.\n",
        "\n",
        "$v{t+1} = \\rho v_t - \\alpha \\nabla f(\\tilde{x}_t)$\n",
        "\n",
        "então,\n",
        "\n",
        "$\\tilde{x}_{t+1} = \\tilde{x}_t +v_{t+1} + \\rho(v_{t+1} - v_t)$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgXXieRQvh4A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def nesterov(args):\n",
        "  dx = compute_gradient(x)\n",
        "  old_v = v\n",
        "  v = rho * v - learning_rate * dx\n",
        "  x += -rho * old_v + (1 + rho) * v"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLFFASYPsJAa",
        "colab_type": "text"
      },
      "source": [
        "* Por que nada novo (entre *momentum*/NAG)?\n",
        "\n",
        "> Como indicar a taxa de aprendizado e declínio da taxa de aprendizado?\n",
        "\n",
        "> O ideal são as taxa de aprendizado adaptativo.\n",
        "\n",
        "## Propagação resiliente (Rprop)\n",
        "\n",
        "* Riedmiller e Braun, 1993;\n",
        "\n",
        "* Derecionado ao problema da taxa de aprendizado adaptativo\n",
        "\n",
        "* Aumenta a taxa de aprendizado por pesos multiplicativamente se os sinais dos dois gradientes anteriores concordarem. Senão, diminui a taxa de aprendizado multiplicativamente.\n",
        "\n",
        "## Rprop update\n",
        "\n",
        "* if $f'_t f'_{t-1} > 0: v_t = \\eta^+ v{t-1}$\n",
        "\n",
        "* else if $f'_t f'_{t-1} < 0: v_t = \\eta^- v_{t-1}$\n",
        "\n",
        "* else: $v_t = v_t$\n",
        "\n",
        "$\\theta_{t+1} = \\theta_t - v_t$\n",
        "\n",
        "$0 < \\eta^- < 1 < \\eta^+$\n",
        "\n",
        "> Onde $\\theta$ é o parâmetro de network; $f$ a função de parâmetro de network; $t$ é o número de interações; $\\alpha$ é o tamanho do passo/taxa de aprendizado; e o $mu$ o momentum.\n",
        "\n",
        "## Rprop inicialização\n",
        "\n",
        "* Inicia todas as atualizações com a iteração 0 até um valor constante.\n",
        "> Se você indicar as duas taxas de aprendizado igual a 1, temos a regra de atualização de Manhattan.\n",
        "\n",
        "$v_o = \\delta$\n",
        "\n",
        "* Rprop divide efetivamente o gradiente pela sua magnitude.\n",
        "> Nunca atualize usando o gradiente, mas  isso pode ser um sinal.\n",
        "\n",
        "## Problemas com Rprop\n",
        "\n",
        "* Considerando um peso que é atualizado em 0.1 em nove *mini-batches* e -0.9 na décima *mini-batch*. SGD poderia manter esse peso aproximadamente onde ele começou, já Rprop iria incrementar o peso nove vezes por $\\delta$ e na décima atualização atualizar diminuindo o peso em $\\delta$;\n",
        "\n",
        "> Atualização efetiva $9 \\delta - \\delta = 8 \\delta$\n",
        "\n",
        "* Dentro de *mini-batches* nos escalamos atualizações bem diferente.\n",
        "\n",
        "## AdaGrad, RMSProp e Adam"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRNCKkTSr_ll",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def adagrad(args):\n",
        "  grad_squared = 0\n",
        "  while True:\n",
        "    dx = compute_gradient(x)\n",
        "    grad_squared += dx * dx # adiciona elementos escalando o gradiente\n",
        "    # baseado no histórico de soma dos quadrados em cada dimensão\n",
        "    x -= learning_rate * dx / (np.sqrt(grad_squared) + 1e-7)\n",
        "\n",
        "def rmsprop(args):\n",
        "  grad_squared = 0\n",
        "  while True:\n",
        "    dx = compute_gradient(x)\n",
        "    grad_squared = decay_rate * grad_squared + (1 - decay_rate) * dx * dx\n",
        "    x -= learning_rate * dx / (np.sqrt(grad_squared) + 1e-7)\n",
        "\n",
        "def adam(args):\n",
        "  first_moment = 0\n",
        "  second_moment = 0\n",
        "  while True:\n",
        "    dx = compute_gradient(x)\n",
        "    first_moment = beta1 * first_moment + (1 - beta1) * dx\n",
        "    second_moment = beta2 * second_moment + (1 - beta2) * dx * dx # Momentum\n",
        "    first_unbias = first_moment / (1 - beta1 ** t)\n",
        "    second_unbias = second_moment / (1 - beta2 ** t) # correção de bias\n",
        "    x -= learning_rate * first_moment / (np.sqrt(second_moment) + 1e-7) # AdaGrad / RMSProp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_jnw-6F54x-",
        "colab_type": "text"
      },
      "source": [
        "* Correção de bias para o fato do primeiro e segundo momento começarem com 0;\n",
        "\n",
        "* Adam com `beta1 = .9`, `beta2 = .999` e `learning_rate = 1e-3` (ou 5e-4) é um bom ponto de start para vários modelos.\n",
        "\n",
        "[ConvNetJS Trainer demo on MNIST](https://cs.stanford.edu/people/karpathy/convnetjs/demo/trainers.html)\n",
        "\n",
        "* SGD, SGD+Momentum, Adagrad, RMSProp, Adam todos possuem **taxa de aprendizado** como hiper-parâmetro, qual é o melhor?\n",
        "\n",
        "![loss_epoch](loss_epoch.png)\n",
        "\n",
        "* A taxa de aprendizado cai com o tempo.\n",
        "\n",
        "> Decaimento exponencial $\\alpha = \\alpha_0 e^{-kt}$\n",
        "\n",
        "> Decaimento 1/t $\\alpha = \\frac{\\alpha_o}{1 + kt}$\n",
        "\n",
        "![learning](learning)\n",
        "\n",
        "## Otimização de primeira ordem\n",
        "\n",
        "1. Usa gradiente para aproximação linear;\n",
        "\n",
        "2. Passo para minimizar a aproximação\n",
        "\n",
        "![first_order](first_order.png)\n",
        "\n",
        "## Otimização de segunda ordem\n",
        "\n",
        "1. Usa o gradiente e Hessian para formar uma aproximação quadrática;\n",
        "\n",
        "2. Passo para a mínima da aproximação\n",
        "\n",
        "![second_order](second_order)\n",
        "\n",
        "* Expansão de segunda ordem de Taylor:\n",
        "\n",
        "$J(\\theta) \\approx J(\\theta_0) + (\\theta - \\theta_0)^\\top \\nabla_{\\theta}J(\\theta_0) + \\frac{1}{2}(\\theta - \\theta_0)^\\top H(\\theta - \\theta_0)$\n",
        "\n",
        "* Resolvendo para o ponto crítico obtemos a atualização para o parâmetro de Newton:\n",
        "\n",
        "$\\theta^* = \\theta_0 - H^{-1} \\nabla_\\theta J(\\theta_0$)\n",
        "\n",
        "* O interessante sobre essa atualização é que não há hiper-parâmetros e nem taxa de aprendizado.\n",
        "\n",
        "* Isso é ruim porque Hessian tem $\\theta(N^2)$ elementos, ivertendo tem $\\theta(N^3)$ onde $N$ é cerca de 10 ou 100 milhões;\n",
        "\n",
        "* Métodos *Quasi-Newton* (BGFS o mais popular), em vez de inverter o Hessian ($\\theta(N^3)$), aproxima o Hessian invertido com atualizações rank 1 frequentemente (\\theta(N^2)$ cada);\n",
        "\n",
        "* L-BFGS (versão de memória limitada do BFGS): Não salva a inversão inteira do Hessian.\n",
        "\n",
        "> Normalmente funciona bem em uma *batch* inteira no modo determinístico. Por exemplo, se você tem uma função $f(x)$ determinístic então L-BFGS provavelmente trabalhará bem;\n",
        "\n",
        "> Não transfere muito bem para o formato *mini-batch*, tem resultados ruins. Adaptando L-BFGS para altas escalas, o modo estocástico é uma área ativa de pesquisa.\n",
        "\n",
        "## Na prática:\n",
        "\n",
        "* **Adam** é uma ótima opção de escolha padrão na maioria dos casos;\n",
        "\n",
        "* Se você pode usar uma *batch* inteira para atualizar então tente o **L-BFGS** (não esqueça de desativar todas as fontes de ruído).\n",
        "\n",
        "## Babysitting learning\n",
        "\n",
        "## Pesquisa de hiper-parâmetro\n",
        "\n",
        "## Monitorar e visualizar a curva de perda\n",
        "\n",
        "## Monitorar e visualizar a acurácia\n",
        "\n",
        "## Rastrear a proporrção de atualização de pesos / magnitude do peso\n",
        "\n",
        "## Além dos erros de treinamento\n",
        "\n",
        "* Algoritmos bons de otimização ajudam a reduzir o erro de treinamento. Como reduzir o erro em dados novos?\n",
        "\n",
        "* Como melhorar o desempenho de modelos singulares? **Regularização**\n",
        "\n",
        "---\n",
        "\n",
        "## Otimização de parâmetros em redes neurais\n",
        "\n",
        "* Em *machine learning*, se começa definindo uma atividade e um modelo. O modelo consiste em uma arquitetura e em parâmetros. Para uma determinada arquitetura, os valores dos parâmetros determinam quão acurado o modelo executa a tarefa;\n",
        "\n",
        "* Mas como se encontram bons valores? Definindo uma função de perda que avalia quão bem o modelo executa. O objetivo é minimizar a perda e assim encontrar valores de parâmetros que correspondem as predições com realismo.\n",
        "\n",
        "### 1. Criando um problema de otimização\n",
        "\n",
        "* A função de perda será diferente em diferentes atividades dependendo do que se quer como *output*. Como se define isso tem grande influência no quanto o modelo irá treinar e executar. Vejamos dois exemplos:\n",
        "\n",
        "#### 1.1 Predição do preço de casas\n",
        "\n",
        "* Digamos que sua atividade é predizer o preço de casas $y \\in \\mathbb{R}$ baseado em informações como área do piso, número de quartos, altura do teto. O quadrado da função de perda pode ser resumido por:\n",
        "\n",
        "> Dados informações sobre a casa, o quadrado da diferença entre a predição e o preço real the que ser o menor possível.\n",
        "\n",
        "$\\mathcal{L} = ||y - \\hat{y}||^2_2$\n",
        "\n",
        "> Onde $\\hat{y}$ é o preço predito e $y$ o preço real, conhecido como **verdade fundamental** (*ground truth*).\n",
        "\n",
        "#### 1.2 Localização de objeto\n",
        "\n",
        "* Num exemplo mais complexo precisamos encontrar um carro numa imagem que contém um. A função de perda é:\n",
        "\n",
        "> Dada uma imagem contendo um carro, predizer qual caixa delimitadora (**bbox**) contém o carro. A caixa predita deve satisfazer o tamanho e a posição o carro verdadeiro o mais próximo possível.\n",
        "\n",
        "$\\mathcal{L} = (x - \\hat{x})^2 + (y - \\hat{y})^2 + (w - \\hat{w})^2 + (h - \\hat{h})^2$\n",
        "\n",
        "> Centro do BBox: $(x - \\hat{x})^2 + (y - \\hat{y})^2$\n",
        "\n",
        "> Largura/Altura do BBox: $(w - \\hat{w})^2 + (h - \\hat{h})^2$\n",
        "\n",
        "* A função de perda depende de:\n",
        "> 1. A predição do modelo depende dos valores do parâmetros (pesos) assim como as entradas (nessa caso, a imagem);\n",
        "> 2. A **verdade fundamental** correspondente a entrada (rótulos, nesse caso caixas delimitadas).\n",
        "\n",
        "### Função de custo\n",
        "\n",
        "* Veja que a perda $\\mathcal{L}$ recebe como entrada um exemplo único, então minimizar isso não garante parâmetros melhore para o modelo para os outros exemplos;\n",
        "\n",
        "* É comum minimizar a média das perdas computadas em todo o dado de treinamento;\n",
        "\n",
        "$\\mathcal{J} = \\frac{1}{m} \\sum^m_{i=1} \\mathcal{L}^{(i)}$\n",
        "\n",
        "> Nós chamamos essa função de **custo**, onde $m$ é o tamanho dos dados de treinamento e $\\mathcal{L}^{(i)}$ é a perda de um único dado de treinamento exemplo $x^{(i)}$ rotulado de $y^{(i)}.\n",
        "\n",
        "### Visualizando a função de custo\n",
        "\n",
        "* Para uma dada quantidade de exemplos e seus respectivos rótulos, a função de custo tem uma paisagem (gráfico) que varia como uma função do parâmetro da rede;\n",
        "\n",
        "* È difícil visualizar essa paisagem, se existem mais de dois parâmetros. No entanto, essa paisagem existe e nosso objetivo é encontrar o ponto onde o valor da função de custo é (aproximadamente) mínimo;\n",
        "\n",
        "* Atualizar os valores dos parâmetros vai mover o valor para mais próximo ou mais distante do nosso ponto mínimo alvo.\n",
        "\n",
        "### O modelo contra a função de custo\n",
        "\n",
        "* É importante distinguir a função $f$ que vai executar a ativadade (o modelo) e dá de saída um rótulo (como um **bbox** para um carro). Isso é definido pela arquitetura e os parâmetros, aproximando uma **função real** que executa a atividade. Valors otimizados de parâmetros vão permitir o modelo de executar a atividade com relativa acurácia;\n",
        "\n",
        "* A função de custo tem como entrada os parâmetros e como saída o custo, avaliando quão bom os parâmetros são para realizar a atividade (nos dados de treinamento).\n",
        "\n",
        "### Otimizando a função de custo\n",
        "\n",
        "* Inicialmente não se sabe valores bons de parâmetro. No entando, temos a formula da funçao de custo. Minimizar a função de custo em teoria lhe ajudará a encontrar bons valores de parâmetros. O jeito de fazer isso é dar dados de treinamento ao modelo e ajustar os parâmetros interativamente para diminuir a função de custo o máximo possível;\n",
        "\n",
        "* Em resumo, o jeito que se define a função de custo vai ditar o desempenho do modelo para a atividade. O diagrama abaixo ilustra o processo para encontrar o modelo que tem bom desempenho.\n",
        "\n",
        "![car_diagram](car_diagram.png)\n",
        "\n",
        "\n",
        "## Rodando o processo de otimização\n",
        "\n",
        "* Para encontrar valores de parâmetro que obtenham o mínimo da função, podemos derivar uma solução de **forma aproximada** (*closed form*) por algebra ou aproximar isso usando um método interativo;\n",
        "\n",
        "* Em *machine learning*, métodos interativos como **gradiente descedente** são frequentemente a única opção pela função de custo ser dependente de um grande número de variáveis e não nunca ter uma forma prática de encontrar uma forma aproximada de solução para o mínimo;\n",
        "\n",
        "* Para o gradiente descedente, devemos inicializar os valores de parâmetro para ter um ponto de otimização. Então, se ajusta os valores de parâmetro interativamente para reduzir o valor da função de custo. Para cada interação, os valores de parâmetro são ajustados de acordo com a direção oposta do gradiente de custo (reduzindo custo);\n",
        "\n",
        "$for\\ x\\ in\\ dataset: $ - predição\n",
        "\n",
        "$\\hat{y} = model_W(x)$ - atualização dos parâmetros\n",
        "\n",
        "$W = W - \\alpha \\frac{ \\partial  \\mathcal{J}(y, \\hat{y})}{\\partial{W}}$\n",
        "\n",
        "> Onde $\\hat{y}$ é a predição do modelo, dado a entrada $x$; $W$ é o parâmetro; $\\frac{\\partial \\mathcal{J}}{\\partial W}$ é o gradiente indicando a direção do $W$ para diminuir $\\mathcal{J}$; $\\alpha$ é a taxa de aprendizado onde é possível mexer para indicar o quanto você quer ajustar o valor de $W$ por interação.\n",
        "\n",
        "* Veja que $\\mathcal{J}$ recebe todo o dado de entrada e computar tudo isso poder ser lento. É normal minimizar a média da perda computada a partir de um conjunto de exemplos; para cada instância, $\\mathcal{J}_{mini - batch} = \\frac{1}{m_b} \\sum^{m_b}_{i=1} \\mathcal{L}^{(i)}$, reduzir essa função gera uma rápida atualização na direção de minimizar o **erro de treinamento**; $m_b$ é o **tamanho da amostra** (o parâmetro chave para tunar).\n",
        "\n",
        "### Ajustando os hiper-parametros do gradiente descedente\n",
        "\n",
        "* Para usar gradiente descendente devemos escolher valores para hiperparâmetros como taxa de aprendizado e tamanho da *batch*. Esses valores vão influenciar a otimização, então é importante escolhe-los apropriadamente.\n",
        "\n",
        "> No site existe uma visualização para descobrir os parâmetros usados para criar os dados, usando o ponto de inicialização, taxa de aprendizado e tamanho da *batch*.\n",
        "\n",
        "* Alguns aprendizados da visualização:\n",
        "> 1. Mesmo escolhendo os melhores hiper-parametros, o modelo de treinamento nao vai corresponder exatamente a **verdade fundamental** (linhas azuis) porque os dados são uma representação da distribuição da **verdade fundamental**;\n",
        "> 2. Quanto maior os dados de treinamento, mais próximo os parâmetros do seu modelo treinado estarão dos parâmetros usados para gerar os dados;\n",
        "> 3. Se a taxa de aprendizdo é muito larga, seu algoritmo não irá covergir. Mas se é baixa ele irá covergir lentamente;\n",
        "> 4. Se o ponto inicial (ponto vermelho) está perto da **verdade fundamental** e os hiper-parametros (taxa de aprendizado e tamanho da *batch*) são *tunados* apropriadamente, seu algoritmo irá covergir rapidamente.\n",
        "\n",
        "### Inicialização\n",
        "\n",
        "* Uma boa inicialização pode acelerar a otimização e permitir a convergência para um mínimo ou o melhor de vários mínimos.\n",
        "\n",
        "* [Initializing Neural Networks](http://www.deeplearning.ai/ai-notes/initialization/)\n",
        "\n",
        "### Taxa de aprendizado\n",
        "\n",
        "* A taxa de aprendizado influencia a convergência da otimização, isso contrabalança a influência da função de custo na curvatura;\n",
        "\n",
        "* De acordo com o gradiente descedente, a direção e magnitude da atualização do parâmetro são dados pela taa de aprendizado multiplicada pela inclinação da função custo num certo ponto $W$, especialmente: $\\alpha \\frac{\\partial \\mathcal{J}}{\\partial W}$\n",
        "\n",
        "> Se a taxa de aprendizado é pequena, atualizações são pequans e otimizações são lentas, especialmente se a curvatura de custo é baixa. Também é possível se acomodar num mínimo local ou plator;\n",
        "\n",
        "> Se a taxa de aprendizado é muito alta, atualizações são altas e otimizações tendem a divergir, especialmente se a curvatura da função de custo\n",
        "é alta;\n",
        "\n",
        "> Se a taxa de aprendizado é escolhida bem, atualizações são apropriadas e a otimização tende a convergir para um bom grupo de parâmetros.\n",
        "\n",
        "* No site tem-se uma visualização para encontrar o parâmetro correspondente ao custo mínimo usando gradiente descendente. O que ela ilustra:\n",
        "> 1. O que faz uma boa taxa de aprendizado depende da curvatura da função custo;\n",
        "> 2. O gradiente descendente cria uma aproximação linear da função custo num dado ponto. Então se move descendo e se aproximando da função custo;\n",
        "> 3. Se o custo tem alta curvatura, quanto mais alto a taxa de aprendizado (passo) mais fácil o algoritmo ultrapassa;\n",
        "> 4. Pequenos passos reduzem o problema anterior, mas diminui o aprendizado.\n",
        "\n",
        "* É coum começar com uma alta taxa de aprendizado (entre 0.1 e 1) e ir diminuindo durante o treinamento. Escolhendo a diminuição (quão frequente? por quanto?) não é simples. Uma diminuição agressiva diminui o progresso em ireção ao ótimo, enquanto uma diminuição regular causa uma atualização caótica mas com pequenas melhoras.\n",
        "\n",
        "* De fato, achando a melhor taxa de diminuição não é simples. N entanto, algoritmos de adaptação de taxa de aprendizado como **Momentum Adam** e **RMSprop** ajudam a ajustar a taxa de aprendizado durante o processo de  otimização.\n",
        "\n",
        "## *Batch size*\n",
        "\n",
        "* *Batch size* é o número de pontos nos dados usados para treinar um modelo em cada interação. *Batchs* tipicamente pequenas são: 32, 64, 128, 256, 512; enquanto grandes *batchs* podem ser centenas de eexemplos.\n",
        "\n",
        "* Escolher o tamanh certo da *batch* é importante para garantir convergência da função custo e dos valores dos parâmetros, além da generalização do modelo. Algumas pesquisas indicam como fazer a escolha, mas não há consensus. Na prática se usa a **pesquisa por parâmetros**.\n",
        "\n",
        "> O tamanho da *batch* determina a frequência de atualizações. Quanto menor a *batch* maior a quantidade de velocidade das atualizações;\n",
        "\n",
        "> Quanto maior o tamanho da *batch*, mais acurado o gradiente do custo será em respeito aos parâmetros. Ou seja, a direção da atualização na maioria irá diminuir a inclinação local do custo;\n",
        "\n",
        "> Tendo grandes *batchs*, mas não tão grande que não dê numa GPU, tende a melhorar a eficiência de paralização e pode acelerar o treino;\n",
        "\n",
        "> Alguns autores (Keskar et al., 2016) sugerem que grandes *batchs* podem prejudicar a habilidade do modelo de generalizar, fazendo com que o modelo ache ótimos locais ou platores pobres.\n",
        "\n",
        "* Escolhendo um tamanho de *batch*, existe um balancemento que depende do hardware disponível e a atividade que deve ser feita.\n",
        "\n",
        "## Atualização interativa\n",
        "\n",
        "* Agora que se tem ponto de início, taxa de aprendizado e o tamanho da *batch* é hora de atualizar os parâmetros interativamente para se mover em direção ao mínimo da função custo.\n",
        "\n",
        "* O algoritmo de otimização é uma escolha central, pode-se testar vários otimizadores na visualização do site para ver os pros e contras de cada um.\n",
        "\n",
        "* A ideia na visualização é brincar com os hiperparâmetros para encontras os valores dos parâmetros que minimizam a função custo. É possível escolher a função de custo e o ponto de início da otimização. Não existe modelo explicito, pode-se considerar que achando o mínimo da função custo é equivalente a encontrar o melhor modelo para a atividade. Para ser simples, o modelo só tem dois parâmetros e o tamnho da *batch* sempre é um.\n",
        "\n",
        "## Escolha do otimizador\n",
        "\n",
        "* A escolha do otimizador influencia tanto a velocidade da convergencia e se ocorre. Vária alternativas do gradiente descedente clássico foram desenvolvidas:\n",
        "\n",
        "> **Gradiente descedente estocástico**: Pode usar paralelização com eficiência, mas é lento quando os dados são tão grandes que a GPU não consegue lidar. Usualmente converge mais rápido que o gradiente descedente clássico em grandes dados, porque atualizações são mais frequentes. É usualmente mais preciso sem usar todos os dados por ser frequentemente redundante. De todas as opções aqui é que usa menos memória.\n",
        "\n",
        "> **Momentum**: Usualmente aumentaa velocidade de aprendizado com poucas mudanças na implementação. Usa mais memória que o gradiente descedente e menos que o RMSprop e Adam;\n",
        "\n",
        "> **RMSprop**: O aprendizado adaptativo dele usualmente previne a diminuição ou aumento brusco da taxa de aprendizado. Ele mantém a taxa de aprendizado de *per-parametros*. Usa mais memória que os citados anteriormente e menos que o Adam;\n",
        "\n",
        "> **Adam**: Os seus hiperparametros são constantes (predefinidos no artigo do algoritmo) não sendo necesário tunar. Ele executa uma taxa de aprendizado por anelamento com passos adaptativos. É o que mais usa memória dos citados, sendo o otimizador padrão em aprendizado de máquina.\n",
        "\n",
        "* Métodos de otimização como **Adam** ou **RMSprop** executam bem na porção inicial do treinamento, mas eles generalizão pocuo nos últimos estágios quando comparados ao **gradiente descedente estocástico**.\n",
        "\n",
        "## Conclusão\n",
        "\n",
        "* Explorar métodos de otimização e valores de hiper-parâmetros podem ajudar a construir intuição para redes de otimização para suas atividades. Durante a pesquisa por hiper-parametros é importante entender intuitivimente a sensibilidade da otimização a taxa de aprendizado, tamanho da *batch*, otimizador, etc. Esse entendimento intuitivo combinado com o método correto (pesquisa aleatória ou otimização bayesiana) vão lhe ajudar a encontrar o modelo certo.\n",
        "\n",
        "## Referências\n",
        "\n",
        "* [Lecture 6 | Training Neural Networks I - 27:25](https://www.youtube.com/watch?v=wEoyxE0GP2M&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=6)\n",
        "\n",
        "* [System optimization in neural networks](http://www.deeplearning.ai/ai-notes/optimization/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gBKHV443rOK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}