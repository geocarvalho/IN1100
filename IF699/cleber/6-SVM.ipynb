{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6-SVM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/geocarvalho/uni-proj/blob/master/IF699/cleber/6-SVM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jw5Ny5cCqs-",
        "colab_type": "text"
      },
      "source": [
        "# Support Vector Machines (SVMs)\n",
        "\n",
        "* Considere um conjunto de *n* pontos $x_i$ (i=1, ..., n) pertencentes a duas classes {+1, -1} linearmente separáveis;\n",
        "\n",
        "* Um classificador pode ser construído a partir de um hiperplano de separação $x.w + b = 0$\n",
        "\n",
        "> Se $x_i.w + b > 0$, então $y_i = +1$;\n",
        "\n",
        "> Se $x_i.w + b < 0$, então $y_i = -1$;\n",
        "\n",
        "> Ou $y_i = sign(x_i.w + b)$\n",
        "\n",
        "* Existem infinitos hiperplanos que separam dois conjuntos de pontos linearmente separáveis. Qual o melhor?\n",
        "\n",
        "> Menor erro de classificação e maior **margem**.\n",
        "\n",
        "* Hiperplano ótimo é equidistante às classes e maximiza a **margem** de separação;\n",
        "\n",
        "![slide7](slide7.png)\n",
        "\n",
        "* Pontos mais próximos do hiperplano ótimo são chamados de **vetores de suporte**;\n",
        "\n",
        "* Hiperplanos **superior** e **inferior** podem ser reescalados para: $x.w + b = +1$ e $x.w + b = -1$\n",
        "\n",
        "> Margem 2d é calculada como $\\frac{2}{|w|}$\n",
        "\n",
        "![2d](2d.png)\n",
        "\n",
        "## Considerando:\n",
        "\n",
        "* Dados de treinamento: tuplas no formato ($x_1, x_2, ..., x_n, y$), atributos $x_i$ e classe $y$ (+1, -1)\n",
        "\n",
        "* Conjunto dito linearmente separável, se existir um hiperplano H (no espaço de entrada) que separe as tuplas de classes diferentes;\n",
        "\n",
        "* Determinar os **vetores de suporte**, encontrar o **hiperplano** ótimo (com maior **margem**)\n",
        "\n",
        "* Considerando os vetores de suporte $x_1$ e $x_2$ temos que a distância entre um hiperplano que toca todos os vetores de suporte do lado de $x_1$ respeitando $w.x + b = +1$, equanto todos os vetores de suporte do lado de $x_2$ respeitam $w.x + b = -1$. Assim, a diferença entre ambas as funções define a **margem** $w.(x_1 - x_2) = 2$\n",
        "\n",
        "> Logo a diferença entre $x_1$ e $x_2$ é dada por $x_1 - x_2 = \\frac{2}{w}$\n",
        "\n",
        "![x1_x2](x1_x2.png)\n",
        "\n",
        "* Como buscamos pela máxima margem ou distância projetada entre $x_1$ e $x_2$, buscamos maximizar $d = x_1 - x_2 = \\frac{2}{|w|}$. Sendo a **distância mínima** entre o hiperplano separador e os dados de treinamento dada por $\\frac{1}{|w|}$. Logo, podemos maximizar esse termo ou o $|w|$ apenas.\n",
        "\n",
        "## Hiperplano ótimo\n",
        "\n",
        "* Aquele que possui a **maior margem** e **menor $|w|$**;\n",
        "\n",
        "* Determinação do hiperplano:\n",
        "> 1. Problema de otimização restrita;\n",
        "> 2. Minimizar uma função de custo (produto interno) sujeito a restrições;\n",
        "> 3. Multiplicadores de *Lagrange*\n",
        "\n",
        "## Multiplicadores de *Lagrange*\n",
        "\n",
        "* Empregado para resolver problemas de extremos sujeitos a restrições de igualdade;\n",
        "\n",
        "* Para $max(min)f(x)$ \n",
        "\n",
        "s.a. $g_i(x) = 0, i=1, ..., N$\n",
        "\n",
        "> Onde $f$ e $g_i(i=1,...,N) são funções reais de $n$ ($n > N$) variáveis que se assumem duas vezes diferenciáveis num determinado conjunto D.\n",
        "\n",
        "> Função de *Lagrange*: $L(x,?) = f(x) + \\sum^N_{i=1}\\lambda_ig_i(x)$\n",
        "\n",
        "* Minimizar: $L_p = \\frac{1}{2}||w||^2 - \\sum_i \\alpha_i \\times [y_i(x_i \\times w + b) -1]$\n",
        "\n",
        "> $L_p$ deve ser minimizada com respeito a $w$ e $b$ e maximizada com respeito a $\\alpha_i$:\n",
        "\n",
        "$max_{\\alpha}{min_{w,b} {L_p(w,b,\\alpha)}}$\n",
        "\n",
        "onde $\\frac{\\partial L_p}{\\partial w}\\ =\\ 0\\ \\to\\ w\\ =\\ \\sum_i\\alpha_ix_iy_i$\n",
        "\n",
        "e $\\frac{\\partial L_p}{\\partial b}\\ =\\ 0\\ \\to\\ w\\ =\\ \\sum_i \\alpha_i y_i$\n",
        "\n",
        "teremos um novo problema de otimização, maximizar:\n",
        "\n",
        "$L_D = \\sum_i \\alpha_i - \\sum_{i,j} \\alpha_i \\times \\alpha_j \\times x_i \\times x_j \\times y_i \\times y_j$\n",
        "\n",
        "> Os vetors $x_i$ e $x_j$ são o vetor de entrada e o padrão de entrada pertecente ao j-ésimo exemplo;\n",
        "\n",
        "> Problema resolvido comumente por métodos de otimização quadrática (SMO) - solução **única e ótima**\n",
        "\n",
        "> Existe um $\\alpha_i$ para cada exemplo de treinamento. Na solução ótima de $L_D, \\alpha_i > 0$ para os vetores de suporte e $\\alpha = 0$ para os outros exemplos;\n",
        "\n",
        "> O hiperplano ótimo depende apenas dos vetores suporte;\n",
        "\n",
        "> Maximizando $L_D$ o hiperplano ótimo é obtido diretamente: $w = \\sum_i \\alpha_i x_i y_i$\n",
        "\n",
        "$b = 1 - w \\times x_{(s)}$ onde $x_{(s)}$ é um vetor suporte no hiperplano superior.\n",
        "\n",
        "## SVM *softmargin*\n",
        "\n",
        "* A formulação anteriormente definida é para conjuntos linearmente separáveis (*Hard Margin SVM*)\n",
        "\n",
        "* Para conjuntos não-linearmente separáveis pequenos erros podem ser tolerados\n",
        "\n",
        "> Minimizar $\\frac{||w||^2}{2} + C \\sum_i \\xi_i$, sujeito a $y_i(x_i.w + b) - 1 + \\xi_i >= 0$\n",
        "\n",
        "* A derivação do *Lagrangiano* introduz apenas uma restrição para $\\alpha_i$\n",
        "\n",
        "$L_D = \\sum_i \\alpha_i - \\sum_{i,j} \\alpha_i \\times \\alpha_j \\times x_i \\times x_j \\times y_i \\times y_j$\n",
        "\n",
        "$0 \\leq \\alpha_i \\leq C$ - valores de $\\alpha_i$ limitados pelo parâmetro de complexidade C.\n",
        "\n",
        "## SVM não-linear\n",
        "\n",
        "* SVM linear ainda é muito limitado mesmo com margens flexíveis;\n",
        "\n",
        "* Generalização não-linear de SVM: Mapear espaço original para espaço não-linear de maior dimensão onde exemplos sejam linearmente separáveis;\n",
        "> Construir **hiperplano ótimo** no novo espaço.\n",
        "\n",
        "## Problema\n",
        "\n",
        "* Como escolher a função $\\Phi (x)$ (transformação) tal que o espaço de características transformado seja eficiente para classificação e não possua custo computacional alto demais?\n",
        "> Com uma função especial, chamada de **função kernel** é possivel calcular o produto escalar $\\Phi (x_i) \\times \\Phi (x_j)$ sem mesmo conhecer o mapeamento $\\Phi$.\n",
        "\n",
        "* Em SVM não-lineares, pontos são mapeados implicitamente através da função kernel, sendo essa função um produto escalar de vetores em algum espaço: $K(x_i, x_j$ = \\Phi(x_i) \\times \\Phi(x_j)$\n",
        "\n",
        "## *The kernel trick*\n",
        "\n",
        "* Parece ingênuo mas pode transformar quaisquer algoritmos lineares que possam ser expressos em termros de produtos internos sem algoritmos não-lineares;\n",
        "\n",
        "* Incrementar o número de dimensões do espaço;\n",
        "\n",
        "* Incrementar muito! Mover o problema para um espaço em que exista uma dimensão independente para cada uma das possíveis entradas de sua função!\n",
        "\n",
        "* O classificador linear depende do produto interno entre exemplos: $K(x_i, x_j) = x_i^{\\top}x_j$\n",
        "\n",
        "* Se cada ponto for mapeado para um espaço de alta dimensão através de uma transformação $\\Phi: x \\to \\varphi(x)$, o produto interno fica: $K(x_i, x_j) = \\varphi(x_i)^{\\top}\\varphi(x_j)$\n",
        "\n",
        "* Uma função de kernel é uma função que é equivalente a um produto interno em um espaço de maior demensionalidade;\n",
        "\n",
        "* Para algumas funções kernel provar que o espaço de transformação existe pode ser difícil. Por construção usando propriedades matemáticas como o Teorema de Mercer é possível.\n",
        "> Toda função simétrica definida semi-positiva é um kernel.\n",
        "\n",
        "* \"O desempeho destes métodos é extremamente dependente da seleção e projeto das funções de kernel utilizadas\";\n",
        "\n",
        "* \"Embora kernels mais gerais apresentem desempenho satisfatório, kernels específicos para diferentes representações de dados pode melhorar bastante os resultados obtidos\";\n",
        "\n",
        "* Não é necessário difinir explicitamente o mapeamento $\\Phi$ (alguns casos é impossível). Todo o treinamento e uso do modelo são realizados apenas usando o kernel;\n",
        "\n",
        "## Funções de kernel mais usadas\n",
        "\n",
        "* Polinomial\n",
        "\n",
        "$K(a,b) = (1 + \\sum_j a_j b_j)^d$\n",
        "\n",
        "* Função de base radial\n",
        "\n",
        "$K(a,b) = exp(\\frac{-(a-b)^2}{2\\sigma^2}$\n",
        "\n",
        "* Saturada próximo de uma sigmóide\n",
        "\n",
        "$K(a,b) = tanh(ca^{\\top}b + h)$\n",
        "\n",
        "* Outros, dependendo do tipo de dado: similaridade de string para texto, algoritmos genéticos, etc.\n",
        "\n",
        "## SVM para múltiplas classes\n",
        "\n",
        "* Uma versos todas: Treina $n$ classificadores binários, um para cada classe contra todas as outras;\n",
        "\n",
        "* Uma versos uma: Treina $\\frac{n(n-1)}{2}$ classificadores, cada um discrimando entre um par de classes. Várias estratégias para seleciona a classificação final se baseando na saída do SVM binário;\n",
        "\n",
        "* SVMs multiclasses: Generaliza a formulação do SVM para multiplas categorias.\n",
        "\n",
        "## Seleção de modelos\n",
        "\n",
        "* A escolha do kernel é importante para o desempneho das SVMs. Dependendo do kernel utilizado alguns parâmetros devem ser definidos, parâmetros de complexidade C é um dos aspectos importantes;\n",
        "\n",
        "* O Kernel **RBF** é mais flexível que o polinomial, ele depende do parâmetro $\\gamma$;\n",
        "> Valores altos dão maior flexibilidade ao modelo mas também aumentam o risco de overfitting.\n",
        "\n",
        "* Sobre o parâmetro **C**: valores muito altos propiciam a geração de modelos mais complexos (risco de overfitting), já valores menores podem aumentar o risco de underfitting.\n",
        "\n",
        "* *Grid-search* separa o conjunto de treinamento e teste. Com o conjunto de treinamento se realiza a validação cruzada para encontrar os melhores valores para $C$ e $\\gamma$. Depois se retreina com o melhor par encontrado e se testa nos dados de teste após o treinamento.\n",
        "\n",
        "## Otimização RNA versos SVM\n",
        "\n",
        "* RNA: mínimo local - Definir a quantidade de neurônios na camada intermediária;\n",
        "\n",
        "* SVM: mínimo global - Definir o melhor parâmetro $C$ (custo);\n",
        "\n",
        "## Comentários\n",
        "\n",
        "* SVM se situa dentre as técnicas de aprendizado mais poderosas, baseada em uma teoria matemática forte. É justificável teoricamente e com bom desempenho empírico;\n",
        "\n",
        "* Apesar de ter poucos parâmetros para selecionar (função kernel), a escolha adequada é importante;\n",
        "\n",
        "* A maior desvantagem é o tempo de treinamento e uso (dependendo da quantidade de classes)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyFWsDAg_49B",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "# Capítulo 26 - Máquinas de vetores de suporte (Support Vector Machines - SVM)\n",
        "\n",
        "## 26.1 Classificador de margem-máxima\n",
        "\n",
        "* O classifidor de margem-máxima é um classificador hipotético que explica muito bem como SVM funciona na prática. As variáveis numéricas de entrada ($x$) nos seus dados (colunas) formam um espaço n-dimensional. Por exemplo, se você tem duas variáveis de entrada, isso formaria um espaço bidimensional;\n",
        "\n",
        "* Um hiperplano é uma linha que divide as variáveis de entrada num espaço. Em SVM, um hiperplano é selecionado para melhor separar os pontos das variáveis de entrada num espaço pela sua classe, classe 0 ou classe 1. Em um espaço bidimensional é possível visualizar isso como uma linha e vamos assumir que todos os pontos de entrada podem ser completamente separáveis por uma linha;\n",
        "\n",
        "* Por exemplo $b_0 + (b_1 \\times x_1) + (b_2 \\times x_2) = 0$\n",
        "\n",
        "> Onde os coeficientes ($b_1$ e $b_2$) que determinam a inclinação da linha e a interceptação ($b_0$) são encontrados pelo algoritmo de apredizagem, sendo $x_1$ e $x_2$ as duas variáveis de entrada;\n",
        "\n",
        "> Classificações podem ser feitas usando essa linha, basta introduzir os valores de entrada na equação da linha que é possível calcular um novo ponto acima ou abaixo da linha.\n",
        "\n",
        "* Abaixo da linha, a equação retorna um valor maior que zero e o ponto pertence a primeira classe (classe 0). Acima da linha, a equação retorna um valor menor que zero e o ponto pertence a segunda classe (classe 1). Já um valor próximo da linha retorna um valor perto de zero e o ponto pode ser difícil de classificar. Se a magnitude do valor é grande, o modelo pode mais confiabilidade na predição.\n",
        "\n",
        "* A distância entre a linha e os dados mais próximos é referenciada como **margem**. A melhor linha ou linha ótima que pode separar duas classes é a linha com a **maior margem** (hiperplano de margem-máxima);\n",
        "\n",
        "* A margem é calculada como a distância perpendicular da linha para os pontos mais próximos. Apenas esses pontos são relevantes na definição da linha e a construção do classificador, esses pontos são chamados de **vetores de suporte**, eles suportam ou definem o hiperplano. O hiperplano é aprendido pelo dados de treinamento usando um processo de otimização que **maximiza** a margem.\n",
        "\n",
        "## 26.2 Classificador de margem suave\n",
        "\n",
        "* Ná prática, dados reais são bagunçados e não podem ser separados por um hiperplano. A restrição de maximizar a margem da linha que separa as classes deve ser relaxada, isso é normalmente chamado de **classificador de margem suave**;\n",
        "\n",
        "* Essa mudança permite que alguns pontos nos dados de treinamento violem a linha de separação. Um grupo adicional de coeficientes são introduzidos dando a margem espaço de manobra em cada dimensão. Esses coeficientes são chamados de variáveis de folga. Isso aumenta a complexidade do modelo havendo mais parâmetros para o modelo caber (*fit*) nos dados e fornecer essa complexidade;\n",
        "\n",
        "* Um parâmetro de afinação $C$ define a magnitude da flexibilidade permitida entre as dimensões. O parâmetro $C$ define a quantidade de violações na margem que são permitidas. Um $C = 0$ significa sem violação e voltamos a inflexibilidade do classificado de margem-máxima descrito anteriormente. Quanto maior o valor de $C$, mais violações no hiperplano são permitidas. Durante o aprendizado do hiperplano a partir dos dados, todas as instâncias de treinamento que estão entre a distância da margem irão afetar o posicionamento do hiperplano e são referidos como **vetores de suporte**. E como $C$ afeta o número de instâncias que permitem cairem dentro da margem, $C$ influencia o número de vetores de suporte usados pelo modelo.\n",
        "\n",
        "> Quanto **menor** o valor de $C$, **mais sensível** o algoritmo é aos dados de treinamento (**maior variância** e **menor viés**);\n",
        "\n",
        "> Quanto **mair** o valor de $C$, **menos sensível** o algoritmo é aos dados de treinamento (**baixa variância** e **maior viés**)\n",
        "\n",
        "## 26.3 Máquinas de suporte de vetores - SVMs (Kernels)\n",
        "\n",
        "* O algoritmo de SVM é implementado na prática usando um *kernel*. O aprendizado do hiperplano numa SVM linear é feito pelo aprendizado do problema usando algebra linear. Um ideia poderosa é que o SVM linear pode ser reformulado usnado o produto interior que qualquer duas observações, em vez das observações diretas;\n",
        "\n",
        "* O produto escalar entre dois vetores é a soma da multiplicação de cada par de valores de entrada. Por exemplo, o produto escalar dos vetores $[2, 3]$ e $[5, 6]$ é $2 \\times 5 + 3 \\times 6$ ou $28$. A equação para fazer a predição para um novo dado de entrada usando o produto escalar entre a entrada ($x$) e cada vetor de suporte ($x_i$) é calculada por:\n",
        "\n",
        "$f(x) = b_0 + \\sum^n_{i=1} (a_i \\times (x \\times x_i))$\n",
        "\n",
        "* Essa é uma equação que envolve calcular o produto escalar de um novo vetor de entrada ($x$) com todos os vetores de suporte nos dados de treinamento. O coeficiente $b_0$ e $a_i$ (para cada entrada) devem ser estimados dos dados de entrada pelo algoritmo de aprendizado.\n",
        "\n",
        "## 26.3.1 SVM *kernel* linear\n",
        "\n",
        "* O produto escalar é chamado de *kernel* e pode ser reescrito como: $K(x,x_i) = \\sum(x \\times x_i)$\n",
        "\n",
        "* O *kernel* define a similaridade ou a medida da distância entre um novo dado e os vetores de suporte. O produto escalar é a medida de similaridade usada no SVM linear ou *kernel* linear porque a distância é a combinação linear dos dados de entrada. Outros *kerneis* podem ser usados para transformar o espaço de entrada em dimensões maiores (chamado de **truque do kernel**) como o **kernel polinomial** e o **kernel radial**.\n",
        "> É desejável usar *kerneis* mais complexos quando se é necessário mais linhas para separar classes que são curvas ou mais complexas, isso pode gerar classificadores mais acurados.\n",
        "\n",
        "## 26.3.2 SVM *kernel* polinomial\n",
        "\n",
        "* Em vez do produto escalar, podemos usar o *kernel* polinomial:\n",
        "\n",
        "$K(x,x_i) = 1 + \\sum(x \\times x_i)^d$\n",
        "\n",
        "* Onde o grau do polinômio pode ser especificado manualmente para um algortimo de aprendizagem. Quando $d = 1$ é o mesmo que um *kernel* linear, o *kernel* polinomial permite linhas curvaos no espaço de entrada.\n",
        "\n",
        "## 26.3.3 SVM *kernel* radial (RBF)\n",
        "\n",
        "* Finalmente, podemos ter o kernel radial mais complexo:\n",
        "\n",
        "$K(x,x_i) = e^{-gamma \\times \\sum ((x-x_i)^2)}$\n",
        "\n",
        "* Onde $gamma$ é o parâmetro que deve ser especificado ao algoritmo de aprendizado. Um bom valor padrão de $gamma$ é 0.1, onde $gamma$ é frequentemente $0 < gamma < 1$;\n",
        "\n",
        "* O *kernel* radial é bem local e pode criar regiões complexas no espaço da *feature*, como aproximações de poligonos num espaço bidimensional.\n",
        "\n",
        "## 26.4 Como aprender um modelo SVM\n",
        "\n",
        "* O modelo SVM precisa ser resolvido unado um processo de otimização. Pode ser usado um processo de otimização numérica para pesquisar pelos coeficientes do hiperplano, isso é ineficiente e não é método mais utilizado nas implementações de SVM como o LIBSVM;\n",
        "\n",
        "* Se for implementar SVM como exercício, pode-se usar uma variação do gradiente descedente chamado **sub-gradiente descedente**;\n",
        "\n",
        "* Existem processos de otimizações especiais que reformulam o problema de otimização para ser um problma de **progamação quadrática**. O método mais popular de SVM é o de **otimização sequencial mínimo** (*Sequencial Minimal Optimization - SMO) que é muito eficient, ele quebra o problema em sub-problemas que podem ser resolvidos analiticamente (por calculo) em vez de numericamente (pesquisando ou otimizando).\n",
        "\n",
        "## 26.5 Preparando os dados para o modelo de SVM\n",
        "\n",
        "* Algumas sugestões:\n",
        "> 1. **Entradas numéricas**: SVM assume que os dados de entrada são numéricos, se você tem dados categoricos é preciso converter em uma variável para cada categoria (*binary dummy variables*);\n",
        "> 2. **Classificação binária**: SVM básica é descrita para uma problema de classificação binária (duas classes). Porém, extensões podem ser desenvolvidas para problemas de regressão e classificação de multiclasses.\n",
        "\n",
        "---\n",
        "\n",
        "# Capítulo 27 - tutorial de SVM\n",
        "\n",
        "* SVM é um algoritmo de aprendizado de máquina flexivel não-paramétrico;\n",
        "\n",
        "* Vamos ver o algoritmo de SVM com o kernel usando sub-gradiente descendente;\n",
        "\n",
        "* Os dados de exemplo foi criado para que as classes fossem linearmente separáveis. Isso significa que uma linha reta pode ser desenhada para separar as classes. Isso foi intensional para que fosse explorado como implementar uma SVM com um kernel linear."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3aIGYb5_4X_",
        "colab_type": "code",
        "outputId": "e7bee706-7013-4138-efcd-eb8423b07f00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = {'X1': [2.327868056, 3.032830419, 4.485465382, 3.684815246, 2.283558563, 7.807521179, 6.132998136, 7.514829366, 5.502385039, 7.432932365], \n",
        "        'X2': [2.458016525, 3.170770366, 3.696728111, 3.846846973, 1.853215997, 3.290132136, 2.140563087, 2.107056961, 1.404002608, 4.236232628], \n",
        "        'Y': [-1, -1, -1, -1, -1, 1, 1, 1, 1, 1]}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>X1</th>\n",
              "      <th>X2</th>\n",
              "      <th>Y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.327868</td>\n",
              "      <td>2.458017</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3.032830</td>\n",
              "      <td>3.170770</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.485465</td>\n",
              "      <td>3.696728</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3.684815</td>\n",
              "      <td>3.846847</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.283559</td>\n",
              "      <td>1.853216</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         X1        X2  Y\n",
              "0  2.327868  2.458017 -1\n",
              "1  3.032830  3.170770 -1\n",
              "2  4.485465  3.696728 -1\n",
              "3  3.684815  3.846847 -1\n",
              "4  2.283559  1.853216 -1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axO43cj1xauC",
        "colab_type": "code",
        "outputId": "1f0c333d-b6e1-450f-dae1-bb5a0315cbf5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.scatter(df[df['Y']==-1]['X1'], df[df['Y']==-1]['X2'], c='b', label='-1')\n",
        "plt.scatter(df[df['Y']==1]['X1'], df[df['Y']==1]['X2'], c='r', label='1')\n",
        "plt.xlim(0, 9)\n",
        "plt.ylim(0, 4.5)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n9AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoBbbEp3yZXV",
        "colab_type": "text"
      },
      "source": [
        "## 27.2 Treinando SVM com gradiente descendente\n",
        "\n",
        "### 27.2.1 Modelo de SVM linear\n",
        "\n",
        "* O modelo de SVM linear é uma linha onde o objetivo do algoritmo é encontrar os melhores valores de coeficientes que separam as classes;\n",
        "\n",
        "$b_0 + (b_1 \\times x_1) + (b_2 \\times x_2) = 0$\n",
        "\n",
        "> Onde $b_0$, $b_1$ e $b_2$ são os coeficientes, $x_1$ e $x_2$ as variáveis de entrada. Essa será a forma da equação que será usada com uma pequena mudança, retiramos o termo de viés ($b_0 = 0$), também chamado *bias* ou *offset* ou *intercept*.  \n",
        "\n",
        "$(b_1 \\times x_1) + (b_2 \\times x_2) = 0$\n",
        "\n",
        "* Isso significa que a linha irá passar na origem ($x_1 = 0$ e $x_2 = 0$). Isso é para deixar o tutorial fácil de ser seguido e porque o exemplo é simples e não precisa disso, podendo ser adicionado se quiser.\n",
        "\n",
        "### 27.2.2 Método de otimização do SVM\n",
        "\n",
        "* O algoritmo de otimização para encontrar o coeficiente pode ser fixado como um problema quadrático. Isso é um tipo de otimização restrita onde soluções rápidas podem ser emplementadas (não usaremos esse modelo no tutorial). Outra forma que pode ser usada para descobrir os valores de coeficiente para o SVM linear é o **sub-gradiente descedente**;\n",
        "\n",
        "* Nesse método um padrão de treinamento aleatório é selecionado cada interação e usado para atualizar os coeficientes. Depois de um grande número de iterações (dezenas ou centenas de dezenas) o algoritmo vai resolver com um grupo estável de coeficientes. A atualização da equação de atualização dos coeficientes funciona assim:\n",
        "\n",
        "1. Primeiro um valor de saída é calculado por: $output = y \\times ((b_1 \\times x_1) + (b_2 \\times x_2))$\n",
        "> Dois procedimentos diferentes de atualização são usados dependendo do valor de saída. \n",
        "\n",
        "2. Se o **valor for > 1** isso sugere que o padrão de treinamento **não foi um vetor de suporte** (isso significa que a instância não foi diretamente envolvida no cálculo do valor de saída) que no caso os pesos são levemente diminuidos por:\n",
        "$b = (1 - \\frac{1}{t}) \\times b$\n",
        "> Onde $b$ é o peso que está sendo atualizado (como $b_1$ ou $b_2$), $t$ é a interação corrente (ex.: 1 para a primeira atualização, 2 para a segunda e assim por seguinte).\n",
        "\n",
        "2. Se o **valor de saída for < 1** então é assumido que a instância de treino é um vetor de suporte e deve ser atualizado para explicar melhor os dados:\n",
        "$b = (1 - \\frac{1}{t}) \\times b + \\frac{1}{lambda \\times t} \\times (y \\times x)$\n",
        "> Onde $b$ é o peso que está sendo atualizado, $t$ é a interação corrente e $lambda$ é o parâmetro para o algoritmo de aprendizado. O $lambda$ é o parâmetro de aprendizado e é frequentemente escolhido com um valor bem baixo como 0.0001 or menor. \n",
        "\n",
        "3. O procedimento é repetido até a taxa de erro cair até um valor desejável ou para um grande número fixo de iterações. \n",
        "> Valores baixos de taxa de aprendizado frequentemente necessitam  de grande tempo de treinamento, o número de iterações é a desvantagem desse algoritmo.\n",
        "\n",
        "## 27.3 Modelo de SVM aprendendo com os dados de treino\n",
        "\n",
        "* Aqui vamos trabalhar com algumas atualizações nos coeficientes para demonstrar o algoritmo de SVM;\n",
        "\n",
        "* Usaremos um $lambda$ alto ($lambda = 0.45$), o que é estranho já que se usa valores muito pequenos, para demonstrar grandes mudanças na atualização.\n",
        "\n",
        "* Inicializamos os coeficientes zerados;\n",
        "\n",
        "* Preciamos acompanhar a iteração ($t = 1$). Vamos treinar o modelo usando a ordem dos padrões do treino. Idealmente, a ordem dos padrões deveria ser aleatória para evitart que o algoritm pare;\n",
        "\n",
        "* O primeiro padrão que usaremos para atualizar é o índice 0 do `df`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9e3aX94NG3gL",
        "colab_type": "code",
        "outputId": "d4b20ec7-3ee4-43d2-80ee-105c64a4fb11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "lamb = 0.45\n",
        "b1 = 0\n",
        "b2 = 0\n",
        "t = 1\n",
        "df.iloc[[0]]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>X1</th>\n",
              "      <th>X2</th>\n",
              "      <th>Y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.327868</td>\n",
              "      <td>2.458017</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         X1        X2  Y\n",
              "0  2.327868  2.458017 -1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ck_2YQw9Hpd4",
        "colab_type": "text"
      },
      "source": [
        "* Podemos calcular o valor de saída para essa iteração:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUVZD51kHnUY",
        "colab_type": "code",
        "outputId": "cdacf993-8784-4d27-c978-04d96aef4bdf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "indice = df.iloc[[0]].values\n",
        "y = indice[0][2]\n",
        "x1 = indice[0][0]\n",
        "x2 = indice[0][1]\n",
        "output = y * ((b1 * x1) + (b2 * x2))\n",
        "output"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJPdIehkJVy4",
        "colab_type": "text"
      },
      "source": [
        "* A saída é < 1, então vamos utilizar a atualização mais complexa que assume que o padrão de treinamento é um vetor de suporte:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFoob1GMJkCr",
        "colab_type": "code",
        "outputId": "eea73208-7a23-459d-ff4b-ecaa42fe737e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "b = output\n",
        "b1 = (1 - 1/t) * b + (1/lamb*t) * (y * x1)\n",
        "b2 = (1 - 1/t) * b + (1/lamb*t) * (y * x2)\n",
        "b1, b2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-5.173040124444444, -5.462258944444445)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzV_8tx_KYy9",
        "colab_type": "text"
      },
      "source": [
        "* Atualizamos os coeficientes que usaremos na próxima iteração ($t = 2$) do algoritmo com a segunda instância dos dados de treinamento (indice 1);\n",
        "\n",
        "* Vamos repetir o processo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yds6tkIZKwNp",
        "colab_type": "code",
        "outputId": "53dc7d98-a2ee-40e5-c71b-ddfd41952d38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "t = 2\n",
        "indice = df.iloc[[1]].values\n",
        "y = indice[0][2]\n",
        "x1 = indice[0][0]\n",
        "x2 = indice[0][1]\n",
        "output = y * ((b1 * x1) + (b2 * x2))\n",
        "output"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "33.00852224058554"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udDTQiv6LMV4",
        "colab_type": "text"
      },
      "source": [
        "* A saída é > 1, sugerindo que a instância do treino não é um vetor de suporte. Atualizamos os coeficientes do modo mais simples:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKkvL962LZF6",
        "colab_type": "code",
        "outputId": "a566e263-a6dc-4b2a-a527-ac40a53c0b26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "b = output\n",
        "b1 = (1 - 1/t) * b1\n",
        "b2 = (1 - 1/t) * b2\n",
        "b1, b2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-2.586520062222222, -2.7311294722222224)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4GeWh-8Lz2E",
        "colab_type": "text"
      },
      "source": [
        "### 27.3.3 Mais iterações\n",
        "\n",
        "* Repitir o processo para o resto dos dados. Uma passagem pelos dados é chamado *epoch*. Agora repetiremos o processo por 15 *epochs* para um total de 160 iterações (16 *epochs* x 10 atualizações por *epoch*);\n",
        "\n",
        "* É possível ficar ciente da perda ou da acurácia do modelo para cada *epoch*. Isso é uma ótima forma de entender quando o algoritmo está convergindo ou quando há um bug na implementação. Se criarmos o gráfico para a acurácia do modelo no fim de cada *epoch*, poderiamos ver algo como:\n",
        "\n",
        "![acc](acc.png)\n",
        "\n",
        "* Você verá que depois de 16 *epochs* que teremos uma acurácia de 100% nos dados de treino. Os valores de coeficientes deveriam ser:\n",
        "\n",
        "$b_1 = 0.552391765$ e $b_2 = -0.724533592$\n",
        "\n",
        "* A forma do hiperplano aprendido é: $0 + (0.552391765 \\times x_1) + (-0.724533592 \\times x_2) = 0$\n",
        "\n",
        "## 27.4 Fazendo predições com o modelo de SVM\n",
        "\n",
        "* Agora que temos os coeficientes da linha, podemos fazer predições. Faremos predições para dados de treino, mas poderia ser adaptado para dados de teste (dados novos):\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdafXaDROGQ_",
        "colab_type": "code",
        "outputId": "67af325b-a791-4408-d4d4-cca6f1a2768b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "source": [
        "def pred(x1, x2):\n",
        "  b1 = 0.52391765\n",
        "  b2 = -0.724533592\n",
        "  output = (b1 * x1) + (b2 * x2)\n",
        "  return output\n",
        "\n",
        "df['output'] = df.apply(lambda x: pred(x['X1'], x['X2']), axis=1)\n",
        "df['crisp'] = df['output'].apply(lambda x: -1 if x < 0 else 1)\n",
        "df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>X1</th>\n",
              "      <th>X2</th>\n",
              "      <th>Y</th>\n",
              "      <th>output</th>\n",
              "      <th>crisp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.327868</td>\n",
              "      <td>2.458017</td>\n",
              "      <td>-1</td>\n",
              "      <td>-0.561304</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3.032830</td>\n",
              "      <td>3.170770</td>\n",
              "      <td>-1</td>\n",
              "      <td>-0.708376</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.485465</td>\n",
              "      <td>3.696728</td>\n",
              "      <td>-1</td>\n",
              "      <td>-0.328389</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3.684815</td>\n",
              "      <td>3.846847</td>\n",
              "      <td>-1</td>\n",
              "      <td>-0.856630</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.283559</td>\n",
              "      <td>1.853216</td>\n",
              "      <td>-1</td>\n",
              "      <td>-0.146321</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>7.807521</td>\n",
              "      <td>3.290132</td>\n",
              "      <td>1</td>\n",
              "      <td>1.706687</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6.132998</td>\n",
              "      <td>2.140563</td>\n",
              "      <td>1</td>\n",
              "      <td>1.662276</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7.514829</td>\n",
              "      <td>2.107057</td>\n",
              "      <td>1</td>\n",
              "      <td>2.410518</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>5.502385</td>\n",
              "      <td>1.404003</td>\n",
              "      <td>1</td>\n",
              "      <td>1.865550</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>7.432932</td>\n",
              "      <td>4.236233</td>\n",
              "      <td>1</td>\n",
              "      <td>0.824952</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         X1        X2  Y    output  crisp\n",
              "0  2.327868  2.458017 -1 -0.561304     -1\n",
              "1  3.032830  3.170770 -1 -0.708376     -1\n",
              "2  4.485465  3.696728 -1 -0.328389     -1\n",
              "3  3.684815  3.846847 -1 -0.856630     -1\n",
              "4  2.283559  1.853216 -1 -0.146321     -1\n",
              "5  7.807521  3.290132  1  1.706687      1\n",
              "6  6.132998  2.140563  1  1.662276      1\n",
              "7  7.514829  2.107057  1  2.410518      1\n",
              "8  5.502385  1.404003  1  1.865550      1\n",
              "9  7.432932  4.236233  1  0.824952      1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4NsSwNFQhU0",
        "colab_type": "text"
      },
      "source": [
        "* Comparando a coluna `crisp` com a coluna `Y` podemos ver que o modelo teve 100% de acurácia nos dados de treinamento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Am7d6Ymt_665",
        "colab_type": "text"
      },
      "source": [
        "# [Machine Learning Tutorial Python - 10 Support Vector Machine (SVM)](https://www.youtube.com/watch?v=FB5EdxAGxQg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnUUx0tZA3qM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVIYCPnkRgiP",
        "colab_type": "code",
        "outputId": "4015a9ba-e63f-454e-bcdb-5fa62872dd24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "iris = load_iris()\n",
        "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "df['target'] = iris.target\n",
        "df['flower_name'] = df.target.apply(lambda x: iris.target_names[x])\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal length (cm)</th>\n",
              "      <th>sepal width (cm)</th>\n",
              "      <th>petal length (cm)</th>\n",
              "      <th>petal width (cm)</th>\n",
              "      <th>target</th>\n",
              "      <th>flower_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sepal length (cm)  sepal width (cm)  ...  target  flower_name\n",
              "0                5.1               3.5  ...       0       setosa\n",
              "1                4.9               3.0  ...       0       setosa\n",
              "2                4.7               3.2  ...       0       setosa\n",
              "3                4.6               3.1  ...       0       setosa\n",
              "4                5.0               3.6  ...       0       setosa\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKckN30zRzD4",
        "colab_type": "code",
        "outputId": "8e5b06a6-aa18-44ba-9603-f3a6ba3c3930",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df0 = df[df.target == 0]\n",
        "df1 = df[df.target == 1]\n",
        "df2 = df[df.target == 2]\n",
        "df1.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal length (cm)</th>\n",
              "      <th>sepal width (cm)</th>\n",
              "      <th>petal length (cm)</th>\n",
              "      <th>petal width (cm)</th>\n",
              "      <th>target</th>\n",
              "      <th>flower_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>7.0</td>\n",
              "      <td>3.2</td>\n",
              "      <td>4.7</td>\n",
              "      <td>1.4</td>\n",
              "      <td>1</td>\n",
              "      <td>versicolor</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>6.4</td>\n",
              "      <td>3.2</td>\n",
              "      <td>4.5</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1</td>\n",
              "      <td>versicolor</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>6.9</td>\n",
              "      <td>3.1</td>\n",
              "      <td>4.9</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1</td>\n",
              "      <td>versicolor</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>5.5</td>\n",
              "      <td>2.3</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.3</td>\n",
              "      <td>1</td>\n",
              "      <td>versicolor</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>6.5</td>\n",
              "      <td>2.8</td>\n",
              "      <td>4.6</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1</td>\n",
              "      <td>versicolor</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    sepal length (cm)  sepal width (cm)  ...  target  flower_name\n",
              "50                7.0               3.2  ...       1   versicolor\n",
              "51                6.4               3.2  ...       1   versicolor\n",
              "52                6.9               3.1  ...       1   versicolor\n",
              "53                5.5               2.3  ...       1   versicolor\n",
              "54                6.5               2.8  ...       1   versicolor\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfT6-f9NS29T",
        "colab_type": "code",
        "outputId": "a1bf8cb3-a69f-407e-c795-bbe751c45f69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "plt.xlabel('sepal length (cm)')\n",
        "plt.ylabel('sepal width (cm)')\n",
        "plt.scatter(df0['sepal length (cm)'], df0['sepal width (cm)'], color='green', marker='+')\n",
        "plt.scatter(df1['sepal length (cm)'], df1['sepal width (cm)'], color='blue', marker='o')\n",
        "plt.scatter(df2['sepal length (cm)'], df2['sepal width (cm)'], color='pink', marker='x')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7f5c9f310780>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEHCAYAAACjh0HiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzhmBmVnN/X92MyfM6B35EwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGJPMV6VTIon",
        "colab_type": "code",
        "outputId": "c6496031-2e76-44e5-d5e8-da8a084b890f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        }
      },
      "source": [
        "plt.xlabel('petal length (cm)')\n",
        "plt.ylabel('petal width (cm)')\n",
        "plt.scatter(df0['petal length (cm)'], df0['petal width (cm)'], color='green', marker='+')\n",
        "plt.scatter(df1['petal length (cm)'], df1['petal width (cm)'], color='blue', marker='o')\n",
        "plt.scatter(df2['petal length (cm)'], df2['petal width (cm)'], color='pink', marker='x')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7f5c9f3107f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpSDJ01ZT8ef",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTAzgZZ0UeVB",
        "colab_type": "code",
        "outputId": "0fe63652-bc5c-42cc-80ba-7593280b1434",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2)\n",
        "X_train.shape, X_test.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((120, 4), (30, 4))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_Dm1pUfUuw0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "svm = SVC(gamma='auto').fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHXB57dBU-Xh",
        "colab_type": "code",
        "outputId": "8cad5d64-fe28-4b2c-d756-7107f745c2e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "svm.score(X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKTxfFX1cqL8",
        "colab_type": "text"
      },
      "source": [
        "## Referencias\n",
        "\n",
        "1. [Machine Learning Tutorial Python - 10 Support Vector Machine (SVM)](https://www.youtube.com/watch?v=FB5EdxAGxQg)\n",
        "2. [Support Vector Machines - The Math of Intelligence (Week 1)](https://www.youtube.com/watch?v=g8D5YL6cOSE)\n",
        "3. [Lecture 67 — Support Vector Machines - Introduction | Stanford University](https://www.youtube.com/watch?v=v7H5ks5iDEQ)\n",
        "4. [Lecture 68 — Support Vector Machines Mathematical Formulation | Stanford](https://www.youtube.com/watch?v=ax8LxRZCORU&t=1s)\n",
        "5. [Lecture 12.1 — Support Vector Machines | Optimization Objective — [ Machine Learning | Andrew Ng]](https://www.youtube.com/watch?v=hCOIMkcsm_g&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=70)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qp6R0oSEVP6i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}